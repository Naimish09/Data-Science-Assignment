{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7MzBbWsu7Emh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"forestfires.csv\", delimiter=\",\")"
      ],
      "metadata": {
        "id": "1K-p_rpk7KOP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "XD3yGSiP7KRA",
        "outputId": "556e8347-4d0f-4ebc-f99d-8727f0b42c89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
              "0   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...         0   \n",
              "1   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...         0   \n",
              "2   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0  ...         0   \n",
              "3   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2  ...         0   \n",
              "4   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0  ...         0   \n",
              "\n",
              "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
              "0         0         0         0         1         0         0         0   \n",
              "1         0         0         0         0         0         0         1   \n",
              "2         0         0         0         0         0         0         1   \n",
              "3         0         0         0         1         0         0         0   \n",
              "4         0         0         0         1         0         0         0   \n",
              "\n",
              "   monthsep  size_category  \n",
              "0         0          small  \n",
              "1         0          small  \n",
              "2         0          small  \n",
              "3         0          small  \n",
              "4         0          small  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7f5fe8-1607-48c6-9f0d-935d8854e2e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>...</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7f5fe8-1607-48c6-9f0d-935d8854e2e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b7f5fe8-1607-48c6-9f0d-935d8854e2e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b7f5fe8-1607-48c6-9f0d-935d8854e2e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0t99t8v7KTa",
        "outputId": "d6d079ce-cb2c-42b0-cf41-a64119ee67ab"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 517 entries, 0 to 516\n",
            "Data columns (total 31 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   month          517 non-null    object \n",
            " 1   day            517 non-null    object \n",
            " 2   FFMC           517 non-null    float64\n",
            " 3   DMC            517 non-null    float64\n",
            " 4   DC             517 non-null    float64\n",
            " 5   ISI            517 non-null    float64\n",
            " 6   temp           517 non-null    float64\n",
            " 7   RH             517 non-null    int64  \n",
            " 8   wind           517 non-null    float64\n",
            " 9   rain           517 non-null    float64\n",
            " 10  area           517 non-null    float64\n",
            " 11  dayfri         517 non-null    int64  \n",
            " 12  daymon         517 non-null    int64  \n",
            " 13  daysat         517 non-null    int64  \n",
            " 14  daysun         517 non-null    int64  \n",
            " 15  daythu         517 non-null    int64  \n",
            " 16  daytue         517 non-null    int64  \n",
            " 17  daywed         517 non-null    int64  \n",
            " 18  monthapr       517 non-null    int64  \n",
            " 19  monthaug       517 non-null    int64  \n",
            " 20  monthdec       517 non-null    int64  \n",
            " 21  monthfeb       517 non-null    int64  \n",
            " 22  monthjan       517 non-null    int64  \n",
            " 23  monthjul       517 non-null    int64  \n",
            " 24  monthjun       517 non-null    int64  \n",
            " 25  monthmar       517 non-null    int64  \n",
            " 26  monthmay       517 non-null    int64  \n",
            " 27  monthnov       517 non-null    int64  \n",
            " 28  monthoct       517 non-null    int64  \n",
            " 29  monthsep       517 non-null    int64  \n",
            " 30  size_category  517 non-null    object \n",
            "dtypes: float64(8), int64(20), object(3)\n",
            "memory usage: 125.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "PkbsOf2I7KXU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "160da3b0-fa99-462e-a54a-c59cce64b7f0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             FFMC         DMC          DC         ISI        temp          RH  \\\n",
              "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
              "mean    90.644681  110.872340  547.940039    9.021663   18.889168   44.288201   \n",
              "std      5.520111   64.046482  248.066192    4.559477    5.806625   16.317469   \n",
              "min     18.700000    1.100000    7.900000    0.000000    2.200000   15.000000   \n",
              "25%     90.200000   68.600000  437.700000    6.500000   15.500000   33.000000   \n",
              "50%     91.600000  108.300000  664.200000    8.400000   19.300000   42.000000   \n",
              "75%     92.900000  142.400000  713.900000   10.800000   22.800000   53.000000   \n",
              "max     96.200000  291.300000  860.600000   56.100000   33.300000  100.000000   \n",
              "\n",
              "             wind        rain         area      dayfri  ...    monthdec  \\\n",
              "count  517.000000  517.000000   517.000000  517.000000  ...  517.000000   \n",
              "mean     4.017602    0.021663    12.847292    0.164410  ...    0.017408   \n",
              "std      1.791653    0.295959    63.655818    0.371006  ...    0.130913   \n",
              "min      0.400000    0.000000     0.000000    0.000000  ...    0.000000   \n",
              "25%      2.700000    0.000000     0.000000    0.000000  ...    0.000000   \n",
              "50%      4.000000    0.000000     0.520000    0.000000  ...    0.000000   \n",
              "75%      4.900000    0.000000     6.570000    0.000000  ...    0.000000   \n",
              "max      9.400000    6.400000  1090.840000    1.000000  ...    1.000000   \n",
              "\n",
              "         monthfeb    monthjan    monthjul    monthjun    monthmar    monthmay  \\\n",
              "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
              "mean     0.038685    0.003868    0.061896    0.032882    0.104449    0.003868   \n",
              "std      0.193029    0.062137    0.241199    0.178500    0.306138    0.062137   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
              "\n",
              "         monthnov    monthoct    monthsep  \n",
              "count  517.000000  517.000000  517.000000  \n",
              "mean     0.001934    0.029014    0.332689  \n",
              "std      0.043980    0.168007    0.471632  \n",
              "min      0.000000    0.000000    0.000000  \n",
              "25%      0.000000    0.000000    0.000000  \n",
              "50%      0.000000    0.000000    0.000000  \n",
              "75%      0.000000    0.000000    1.000000  \n",
              "max      1.000000    1.000000    1.000000  \n",
              "\n",
              "[8 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-965e365d-78ad-47a1-8967-b2148584b2d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>...</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>517.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>90.644681</td>\n",
              "      <td>110.872340</td>\n",
              "      <td>547.940039</td>\n",
              "      <td>9.021663</td>\n",
              "      <td>18.889168</td>\n",
              "      <td>44.288201</td>\n",
              "      <td>4.017602</td>\n",
              "      <td>0.021663</td>\n",
              "      <td>12.847292</td>\n",
              "      <td>0.164410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017408</td>\n",
              "      <td>0.038685</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.061896</td>\n",
              "      <td>0.032882</td>\n",
              "      <td>0.104449</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.029014</td>\n",
              "      <td>0.332689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.520111</td>\n",
              "      <td>64.046482</td>\n",
              "      <td>248.066192</td>\n",
              "      <td>4.559477</td>\n",
              "      <td>5.806625</td>\n",
              "      <td>16.317469</td>\n",
              "      <td>1.791653</td>\n",
              "      <td>0.295959</td>\n",
              "      <td>63.655818</td>\n",
              "      <td>0.371006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.130913</td>\n",
              "      <td>0.193029</td>\n",
              "      <td>0.062137</td>\n",
              "      <td>0.241199</td>\n",
              "      <td>0.178500</td>\n",
              "      <td>0.306138</td>\n",
              "      <td>0.062137</td>\n",
              "      <td>0.043980</td>\n",
              "      <td>0.168007</td>\n",
              "      <td>0.471632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>18.700000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>90.200000</td>\n",
              "      <td>68.600000</td>\n",
              "      <td>437.700000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>91.600000</td>\n",
              "      <td>108.300000</td>\n",
              "      <td>664.200000</td>\n",
              "      <td>8.400000</td>\n",
              "      <td>19.300000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>92.900000</td>\n",
              "      <td>142.400000</td>\n",
              "      <td>713.900000</td>\n",
              "      <td>10.800000</td>\n",
              "      <td>22.800000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>4.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.570000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>96.200000</td>\n",
              "      <td>291.300000</td>\n",
              "      <td>860.600000</td>\n",
              "      <td>56.100000</td>\n",
              "      <td>33.300000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>9.400000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>1090.840000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-965e365d-78ad-47a1-8967-b2148584b2d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-965e365d-78ad-47a1-8967-b2148584b2d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-965e365d-78ad-47a1-8967-b2148584b2d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.corr()"
      ],
      "metadata": {
        "id": "3n3dnu_c7KZO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48631732-aff7-4431-bc7b-6c41cdf1ba36"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              FFMC       DMC        DC       ISI      temp        RH  \\\n",
              "FFMC      1.000000  0.382619  0.330512  0.531805  0.431532 -0.300995   \n",
              "DMC       0.382619  1.000000  0.682192  0.305128  0.469594  0.073795   \n",
              "DC        0.330512  0.682192  1.000000  0.229154  0.496208 -0.039192   \n",
              "ISI       0.531805  0.305128  0.229154  1.000000  0.394287 -0.132517   \n",
              "temp      0.431532  0.469594  0.496208  0.394287  1.000000 -0.527390   \n",
              "RH       -0.300995  0.073795 -0.039192 -0.132517 -0.527390  1.000000   \n",
              "wind     -0.028485 -0.105342 -0.203466  0.106826 -0.227116  0.069410   \n",
              "rain      0.056702  0.074790  0.035861  0.067668  0.069491  0.099751   \n",
              "area      0.040122  0.072994  0.049383  0.008258  0.097844 -0.075519   \n",
              "dayfri    0.019306 -0.012010 -0.004220  0.046695 -0.071949  0.064506   \n",
              "daymon   -0.059396 -0.107921 -0.052993 -0.158601 -0.136529  0.009376   \n",
              "daysat   -0.019637 -0.003653 -0.035189 -0.038585  0.034899 -0.023869   \n",
              "daysun   -0.089517  0.025355 -0.001431 -0.003243  0.014403  0.136220   \n",
              "daythu    0.071730  0.087672  0.051859 -0.022406  0.051432 -0.123061   \n",
              "daytue    0.011225  0.000016  0.028368  0.068610  0.035630 -0.014211   \n",
              "daywed    0.093908  0.017939  0.024803  0.125415  0.090580 -0.087508   \n",
              "monthapr -0.117199 -0.197543 -0.268211 -0.106478 -0.157051  0.021235   \n",
              "monthaug  0.228103  0.497928  0.279361  0.334639  0.351404  0.054761   \n",
              "monthdec -0.137044 -0.176301 -0.105642 -0.162322 -0.329648 -0.047714   \n",
              "monthfeb -0.281535 -0.317899 -0.399277 -0.249777 -0.320015  0.140430   \n",
              "monthjan -0.454771 -0.105647 -0.115064 -0.103588 -0.146520  0.170923   \n",
              "monthjul  0.031833 -0.001946 -0.100887  0.020982  0.142588  0.013185   \n",
              "monthjun -0.040634 -0.050403 -0.186183  0.111516  0.051015  0.009382   \n",
              "monthmar -0.074327 -0.407404 -0.650427 -0.143520 -0.341797 -0.089836   \n",
              "monthmay -0.037230 -0.081980 -0.114209 -0.060493 -0.045540  0.086822   \n",
              "monthnov -0.088964 -0.074218 -0.078380 -0.076559 -0.053798 -0.035885   \n",
              "monthoct -0.005998 -0.187632  0.093279 -0.071154 -0.053513 -0.072334   \n",
              "monthsep  0.076609  0.110907  0.531857 -0.068877  0.088006 -0.062596   \n",
              "\n",
              "              wind      rain      area    dayfri  ...  monthdec  monthfeb  \\\n",
              "FFMC     -0.028485  0.056702  0.040122  0.019306  ... -0.137044 -0.281535   \n",
              "DMC      -0.105342  0.074790  0.072994 -0.012010  ... -0.176301 -0.317899   \n",
              "DC       -0.203466  0.035861  0.049383 -0.004220  ... -0.105642 -0.399277   \n",
              "ISI       0.106826  0.067668  0.008258  0.046695  ... -0.162322 -0.249777   \n",
              "temp     -0.227116  0.069491  0.097844 -0.071949  ... -0.329648 -0.320015   \n",
              "RH        0.069410  0.099751 -0.075519  0.064506  ... -0.047714  0.140430   \n",
              "wind      1.000000  0.061119  0.012317  0.118090  ...  0.269702 -0.029431   \n",
              "rain      0.061119  1.000000 -0.007366 -0.004261  ... -0.009752 -0.014698   \n",
              "area      0.012317 -0.007366  1.000000 -0.052911  ...  0.001010 -0.020732   \n",
              "dayfri    0.118090 -0.004261 -0.052911  1.000000  ... -0.019140  0.046323   \n",
              "daymon   -0.063881 -0.029945 -0.021206 -0.181293  ...  0.114519  0.003933   \n",
              "daysat   -0.063799 -0.032271  0.087868 -0.195372  ... -0.058625  0.020406   \n",
              "daysun    0.027981 -0.017872 -0.020463 -0.210462  ... -0.024966  0.008416   \n",
              "daythu   -0.062553 -0.026798  0.020121 -0.162237  ... -0.002838 -0.042278   \n",
              "daytue    0.053396  0.139311 -0.001333 -0.166728  ... -0.005125 -0.014491   \n",
              "daywed   -0.019965 -0.020744 -0.011452 -0.151487  ...  0.002899 -0.035713   \n",
              "monthapr  0.048266 -0.009752 -0.008280 -0.019140  ... -0.017717 -0.026701   \n",
              "monthaug  0.028577  0.093101 -0.004187 -0.100837  ... -0.098941 -0.149116   \n",
              "monthdec  0.269702 -0.009752  0.001010 -0.019140  ...  1.000000 -0.026701   \n",
              "monthfeb -0.029431 -0.014698 -0.020732  0.046323  ... -0.026701  1.000000   \n",
              "monthjan -0.070245 -0.004566 -0.012589 -0.027643  ... -0.008295 -0.012501   \n",
              "monthjul -0.040645 -0.013390  0.006149 -0.048969  ... -0.034190 -0.051528   \n",
              "monthjun  0.012124 -0.013510 -0.020314  0.006000  ... -0.024543 -0.036989   \n",
              "monthmar  0.181433 -0.020744 -0.045596  0.036205  ... -0.045456 -0.068508   \n",
              "monthmay  0.015054 -0.004566  0.006264  0.056423  ... -0.008295 -0.012501   \n",
              "monthnov  0.011864 -0.003225 -0.008893 -0.019527  ... -0.005860 -0.008831   \n",
              "monthoct -0.053850 -0.012665 -0.016878 -0.045585  ... -0.023008 -0.034676   \n",
              "monthsep -0.181476 -0.051733  0.056573  0.107671  ... -0.093982 -0.141642   \n",
              "\n",
              "          monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
              "FFMC     -0.454771  0.031833 -0.040634 -0.074327 -0.037230 -0.088964   \n",
              "DMC      -0.105647 -0.001946 -0.050403 -0.407404 -0.081980 -0.074218   \n",
              "DC       -0.115064 -0.100887 -0.186183 -0.650427 -0.114209 -0.078380   \n",
              "ISI      -0.103588  0.020982  0.111516 -0.143520 -0.060493 -0.076559   \n",
              "temp     -0.146520  0.142588  0.051015 -0.341797 -0.045540 -0.053798   \n",
              "RH        0.170923  0.013185  0.009382 -0.089836  0.086822 -0.035885   \n",
              "wind     -0.070245 -0.040645  0.012124  0.181433  0.015054  0.011864   \n",
              "rain     -0.004566 -0.013390 -0.013510 -0.020744 -0.004566 -0.003225   \n",
              "area     -0.012589  0.006149 -0.020314 -0.045596  0.006264 -0.008893   \n",
              "dayfri   -0.027643 -0.048969  0.006000  0.036205  0.056423 -0.019527   \n",
              "daymon   -0.025470 -0.013300  0.017553  0.077125 -0.025470 -0.017992   \n",
              "daysat    0.057019  0.060945 -0.022408  0.021024  0.057019 -0.019390   \n",
              "daysun    0.050887 -0.018241  0.024540 -0.047726 -0.029568 -0.020887   \n",
              "daythu   -0.022793 -0.019300 -0.000195 -0.026885 -0.022793 -0.016101   \n",
              "daytue   -0.023424  0.049688 -0.069308 -0.032351 -0.023424  0.117121   \n",
              "daywed   -0.021282 -0.008985  0.043422 -0.033917 -0.021282 -0.015034   \n",
              "monthapr -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
              "monthaug -0.046323 -0.190937 -0.137065 -0.253859 -0.046323 -0.032724   \n",
              "monthdec -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
              "monthfeb -0.012501 -0.051528 -0.036989 -0.068508 -0.012501 -0.008831   \n",
              "monthjan  1.000000 -0.016007 -0.011491 -0.021282 -0.003883 -0.002743   \n",
              "monthjul -0.016007  1.000000 -0.047363 -0.087722 -0.016007 -0.011308   \n",
              "monthjun -0.011491 -0.047363  1.000000 -0.062972 -0.011491 -0.008117   \n",
              "monthmar -0.021282 -0.087722 -0.062972  1.000000 -0.021282 -0.015034   \n",
              "monthmay -0.003883 -0.016007 -0.011491 -0.021282  1.000000 -0.002743   \n",
              "monthnov -0.002743 -0.011308 -0.008117 -0.015034 -0.002743  1.000000   \n",
              "monthoct -0.010772 -0.044402 -0.031874 -0.059034 -0.010772 -0.007610   \n",
              "monthsep -0.044001 -0.181367 -0.130195 -0.241135 -0.044001 -0.031083   \n",
              "\n",
              "          monthoct  monthsep  \n",
              "FFMC     -0.005998  0.076609  \n",
              "DMC      -0.187632  0.110907  \n",
              "DC        0.093279  0.531857  \n",
              "ISI      -0.071154 -0.068877  \n",
              "temp     -0.053513  0.088006  \n",
              "RH       -0.072334 -0.062596  \n",
              "wind     -0.053850 -0.181476  \n",
              "rain     -0.012665 -0.051733  \n",
              "area     -0.016878  0.056573  \n",
              "dayfri   -0.045585  0.107671  \n",
              "daymon    0.060975  0.039632  \n",
              "daysat    0.017584 -0.032783  \n",
              "daysun    0.007252 -0.048817  \n",
              "daythu   -0.063223  0.008984  \n",
              "daytue    0.005008 -0.028570  \n",
              "daywed    0.016325 -0.053222  \n",
              "monthapr -0.023008 -0.093982  \n",
              "monthaug -0.128493 -0.524858  \n",
              "monthdec -0.023008 -0.093982  \n",
              "monthfeb -0.034676 -0.141642  \n",
              "monthjan -0.010772 -0.044001  \n",
              "monthjul -0.044402 -0.181367  \n",
              "monthjun -0.031874 -0.130195  \n",
              "monthmar -0.059034 -0.241135  \n",
              "monthmay -0.010772 -0.044001  \n",
              "monthnov -0.007610 -0.031083  \n",
              "monthoct  1.000000 -0.122053  \n",
              "monthsep -0.122053  1.000000  \n",
              "\n",
              "[28 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-100c48bc-912a-4672-b17d-7c8d7e4e2657\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>...</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>FFMC</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.382619</td>\n",
              "      <td>0.330512</td>\n",
              "      <td>0.531805</td>\n",
              "      <td>0.431532</td>\n",
              "      <td>-0.300995</td>\n",
              "      <td>-0.028485</td>\n",
              "      <td>0.056702</td>\n",
              "      <td>0.040122</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.137044</td>\n",
              "      <td>-0.281535</td>\n",
              "      <td>-0.454771</td>\n",
              "      <td>0.031833</td>\n",
              "      <td>-0.040634</td>\n",
              "      <td>-0.074327</td>\n",
              "      <td>-0.037230</td>\n",
              "      <td>-0.088964</td>\n",
              "      <td>-0.005998</td>\n",
              "      <td>0.076609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DMC</th>\n",
              "      <td>0.382619</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.682192</td>\n",
              "      <td>0.305128</td>\n",
              "      <td>0.469594</td>\n",
              "      <td>0.073795</td>\n",
              "      <td>-0.105342</td>\n",
              "      <td>0.074790</td>\n",
              "      <td>0.072994</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.176301</td>\n",
              "      <td>-0.317899</td>\n",
              "      <td>-0.105647</td>\n",
              "      <td>-0.001946</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.407404</td>\n",
              "      <td>-0.081980</td>\n",
              "      <td>-0.074218</td>\n",
              "      <td>-0.187632</td>\n",
              "      <td>0.110907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DC</th>\n",
              "      <td>0.330512</td>\n",
              "      <td>0.682192</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.229154</td>\n",
              "      <td>0.496208</td>\n",
              "      <td>-0.039192</td>\n",
              "      <td>-0.203466</td>\n",
              "      <td>0.035861</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>-0.399277</td>\n",
              "      <td>-0.115064</td>\n",
              "      <td>-0.100887</td>\n",
              "      <td>-0.186183</td>\n",
              "      <td>-0.650427</td>\n",
              "      <td>-0.114209</td>\n",
              "      <td>-0.078380</td>\n",
              "      <td>0.093279</td>\n",
              "      <td>0.531857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISI</th>\n",
              "      <td>0.531805</td>\n",
              "      <td>0.305128</td>\n",
              "      <td>0.229154</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.394287</td>\n",
              "      <td>-0.132517</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>0.067668</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.046695</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.162322</td>\n",
              "      <td>-0.249777</td>\n",
              "      <td>-0.103588</td>\n",
              "      <td>0.020982</td>\n",
              "      <td>0.111516</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>-0.060493</td>\n",
              "      <td>-0.076559</td>\n",
              "      <td>-0.071154</td>\n",
              "      <td>-0.068877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>temp</th>\n",
              "      <td>0.431532</td>\n",
              "      <td>0.469594</td>\n",
              "      <td>0.496208</td>\n",
              "      <td>0.394287</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.527390</td>\n",
              "      <td>-0.227116</td>\n",
              "      <td>0.069491</td>\n",
              "      <td>0.097844</td>\n",
              "      <td>-0.071949</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.329648</td>\n",
              "      <td>-0.320015</td>\n",
              "      <td>-0.146520</td>\n",
              "      <td>0.142588</td>\n",
              "      <td>0.051015</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>-0.045540</td>\n",
              "      <td>-0.053798</td>\n",
              "      <td>-0.053513</td>\n",
              "      <td>0.088006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RH</th>\n",
              "      <td>-0.300995</td>\n",
              "      <td>0.073795</td>\n",
              "      <td>-0.039192</td>\n",
              "      <td>-0.132517</td>\n",
              "      <td>-0.527390</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.069410</td>\n",
              "      <td>0.099751</td>\n",
              "      <td>-0.075519</td>\n",
              "      <td>0.064506</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047714</td>\n",
              "      <td>0.140430</td>\n",
              "      <td>0.170923</td>\n",
              "      <td>0.013185</td>\n",
              "      <td>0.009382</td>\n",
              "      <td>-0.089836</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>-0.035885</td>\n",
              "      <td>-0.072334</td>\n",
              "      <td>-0.062596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wind</th>\n",
              "      <td>-0.028485</td>\n",
              "      <td>-0.105342</td>\n",
              "      <td>-0.203466</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.227116</td>\n",
              "      <td>0.069410</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.061119</td>\n",
              "      <td>0.012317</td>\n",
              "      <td>0.118090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269702</td>\n",
              "      <td>-0.029431</td>\n",
              "      <td>-0.070245</td>\n",
              "      <td>-0.040645</td>\n",
              "      <td>0.012124</td>\n",
              "      <td>0.181433</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>0.011864</td>\n",
              "      <td>-0.053850</td>\n",
              "      <td>-0.181476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rain</th>\n",
              "      <td>0.056702</td>\n",
              "      <td>0.074790</td>\n",
              "      <td>0.035861</td>\n",
              "      <td>0.067668</td>\n",
              "      <td>0.069491</td>\n",
              "      <td>0.099751</td>\n",
              "      <td>0.061119</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.007366</td>\n",
              "      <td>-0.004261</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>-0.014698</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>-0.013510</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.003225</td>\n",
              "      <td>-0.012665</td>\n",
              "      <td>-0.051733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area</th>\n",
              "      <td>0.040122</td>\n",
              "      <td>0.072994</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.097844</td>\n",
              "      <td>-0.075519</td>\n",
              "      <td>0.012317</td>\n",
              "      <td>-0.007366</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.052911</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>-0.020732</td>\n",
              "      <td>-0.012589</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>-0.020314</td>\n",
              "      <td>-0.045596</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>-0.008893</td>\n",
              "      <td>-0.016878</td>\n",
              "      <td>0.056573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dayfri</th>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.046695</td>\n",
              "      <td>-0.071949</td>\n",
              "      <td>0.064506</td>\n",
              "      <td>0.118090</td>\n",
              "      <td>-0.004261</td>\n",
              "      <td>-0.052911</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>0.046323</td>\n",
              "      <td>-0.027643</td>\n",
              "      <td>-0.048969</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.036205</td>\n",
              "      <td>0.056423</td>\n",
              "      <td>-0.019527</td>\n",
              "      <td>-0.045585</td>\n",
              "      <td>0.107671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daymon</th>\n",
              "      <td>-0.059396</td>\n",
              "      <td>-0.107921</td>\n",
              "      <td>-0.052993</td>\n",
              "      <td>-0.158601</td>\n",
              "      <td>-0.136529</td>\n",
              "      <td>0.009376</td>\n",
              "      <td>-0.063881</td>\n",
              "      <td>-0.029945</td>\n",
              "      <td>-0.021206</td>\n",
              "      <td>-0.181293</td>\n",
              "      <td>...</td>\n",
              "      <td>0.114519</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>-0.013300</td>\n",
              "      <td>0.017553</td>\n",
              "      <td>0.077125</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>-0.017992</td>\n",
              "      <td>0.060975</td>\n",
              "      <td>0.039632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daysat</th>\n",
              "      <td>-0.019637</td>\n",
              "      <td>-0.003653</td>\n",
              "      <td>-0.035189</td>\n",
              "      <td>-0.038585</td>\n",
              "      <td>0.034899</td>\n",
              "      <td>-0.023869</td>\n",
              "      <td>-0.063799</td>\n",
              "      <td>-0.032271</td>\n",
              "      <td>0.087868</td>\n",
              "      <td>-0.195372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058625</td>\n",
              "      <td>0.020406</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>0.060945</td>\n",
              "      <td>-0.022408</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>-0.019390</td>\n",
              "      <td>0.017584</td>\n",
              "      <td>-0.032783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daysun</th>\n",
              "      <td>-0.089517</td>\n",
              "      <td>0.025355</td>\n",
              "      <td>-0.001431</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>0.136220</td>\n",
              "      <td>0.027981</td>\n",
              "      <td>-0.017872</td>\n",
              "      <td>-0.020463</td>\n",
              "      <td>-0.210462</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024966</td>\n",
              "      <td>0.008416</td>\n",
              "      <td>0.050887</td>\n",
              "      <td>-0.018241</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>-0.047726</td>\n",
              "      <td>-0.029568</td>\n",
              "      <td>-0.020887</td>\n",
              "      <td>0.007252</td>\n",
              "      <td>-0.048817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daythu</th>\n",
              "      <td>0.071730</td>\n",
              "      <td>0.087672</td>\n",
              "      <td>0.051859</td>\n",
              "      <td>-0.022406</td>\n",
              "      <td>0.051432</td>\n",
              "      <td>-0.123061</td>\n",
              "      <td>-0.062553</td>\n",
              "      <td>-0.026798</td>\n",
              "      <td>0.020121</td>\n",
              "      <td>-0.162237</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002838</td>\n",
              "      <td>-0.042278</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.019300</td>\n",
              "      <td>-0.000195</td>\n",
              "      <td>-0.026885</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.016101</td>\n",
              "      <td>-0.063223</td>\n",
              "      <td>0.008984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daytue</th>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>0.068610</td>\n",
              "      <td>0.035630</td>\n",
              "      <td>-0.014211</td>\n",
              "      <td>0.053396</td>\n",
              "      <td>0.139311</td>\n",
              "      <td>-0.001333</td>\n",
              "      <td>-0.166728</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005125</td>\n",
              "      <td>-0.014491</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>0.049688</td>\n",
              "      <td>-0.069308</td>\n",
              "      <td>-0.032351</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>0.117121</td>\n",
              "      <td>0.005008</td>\n",
              "      <td>-0.028570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>daywed</th>\n",
              "      <td>0.093908</td>\n",
              "      <td>0.017939</td>\n",
              "      <td>0.024803</td>\n",
              "      <td>0.125415</td>\n",
              "      <td>0.090580</td>\n",
              "      <td>-0.087508</td>\n",
              "      <td>-0.019965</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.011452</td>\n",
              "      <td>-0.151487</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>-0.035713</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.008985</td>\n",
              "      <td>0.043422</td>\n",
              "      <td>-0.033917</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>0.016325</td>\n",
              "      <td>-0.053222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthapr</th>\n",
              "      <td>-0.117199</td>\n",
              "      <td>-0.197543</td>\n",
              "      <td>-0.268211</td>\n",
              "      <td>-0.106478</td>\n",
              "      <td>-0.157051</td>\n",
              "      <td>0.021235</td>\n",
              "      <td>0.048266</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>-0.008280</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017717</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.093982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthaug</th>\n",
              "      <td>0.228103</td>\n",
              "      <td>0.497928</td>\n",
              "      <td>0.279361</td>\n",
              "      <td>0.334639</td>\n",
              "      <td>0.351404</td>\n",
              "      <td>0.054761</td>\n",
              "      <td>0.028577</td>\n",
              "      <td>0.093101</td>\n",
              "      <td>-0.004187</td>\n",
              "      <td>-0.100837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.098941</td>\n",
              "      <td>-0.149116</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.190937</td>\n",
              "      <td>-0.137065</td>\n",
              "      <td>-0.253859</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.032724</td>\n",
              "      <td>-0.128493</td>\n",
              "      <td>-0.524858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthdec</th>\n",
              "      <td>-0.137044</td>\n",
              "      <td>-0.176301</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>-0.162322</td>\n",
              "      <td>-0.329648</td>\n",
              "      <td>-0.047714</td>\n",
              "      <td>0.269702</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.093982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthfeb</th>\n",
              "      <td>-0.281535</td>\n",
              "      <td>-0.317899</td>\n",
              "      <td>-0.399277</td>\n",
              "      <td>-0.249777</td>\n",
              "      <td>-0.320015</td>\n",
              "      <td>0.140430</td>\n",
              "      <td>-0.029431</td>\n",
              "      <td>-0.014698</td>\n",
              "      <td>-0.020732</td>\n",
              "      <td>0.046323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.051528</td>\n",
              "      <td>-0.036989</td>\n",
              "      <td>-0.068508</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.008831</td>\n",
              "      <td>-0.034676</td>\n",
              "      <td>-0.141642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthjan</th>\n",
              "      <td>-0.454771</td>\n",
              "      <td>-0.105647</td>\n",
              "      <td>-0.115064</td>\n",
              "      <td>-0.103588</td>\n",
              "      <td>-0.146520</td>\n",
              "      <td>0.170923</td>\n",
              "      <td>-0.070245</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.012589</td>\n",
              "      <td>-0.027643</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.003883</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthjul</th>\n",
              "      <td>0.031833</td>\n",
              "      <td>-0.001946</td>\n",
              "      <td>-0.100887</td>\n",
              "      <td>0.020982</td>\n",
              "      <td>0.142588</td>\n",
              "      <td>0.013185</td>\n",
              "      <td>-0.040645</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>-0.048969</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.051528</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>-0.087722</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011308</td>\n",
              "      <td>-0.044402</td>\n",
              "      <td>-0.181367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthjun</th>\n",
              "      <td>-0.040634</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.186183</td>\n",
              "      <td>0.111516</td>\n",
              "      <td>0.051015</td>\n",
              "      <td>0.009382</td>\n",
              "      <td>0.012124</td>\n",
              "      <td>-0.013510</td>\n",
              "      <td>-0.020314</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.036989</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062972</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.008117</td>\n",
              "      <td>-0.031874</td>\n",
              "      <td>-0.130195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthmar</th>\n",
              "      <td>-0.074327</td>\n",
              "      <td>-0.407404</td>\n",
              "      <td>-0.650427</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>-0.089836</td>\n",
              "      <td>0.181433</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.045596</td>\n",
              "      <td>0.036205</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.068508</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.087722</td>\n",
              "      <td>-0.062972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>-0.059034</td>\n",
              "      <td>-0.241135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthmay</th>\n",
              "      <td>-0.037230</td>\n",
              "      <td>-0.081980</td>\n",
              "      <td>-0.114209</td>\n",
              "      <td>-0.060493</td>\n",
              "      <td>-0.045540</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>0.056423</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.003883</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthnov</th>\n",
              "      <td>-0.088964</td>\n",
              "      <td>-0.074218</td>\n",
              "      <td>-0.078380</td>\n",
              "      <td>-0.076559</td>\n",
              "      <td>-0.053798</td>\n",
              "      <td>-0.035885</td>\n",
              "      <td>0.011864</td>\n",
              "      <td>-0.003225</td>\n",
              "      <td>-0.008893</td>\n",
              "      <td>-0.019527</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.008831</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.011308</td>\n",
              "      <td>-0.008117</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.007610</td>\n",
              "      <td>-0.031083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthoct</th>\n",
              "      <td>-0.005998</td>\n",
              "      <td>-0.187632</td>\n",
              "      <td>0.093279</td>\n",
              "      <td>-0.071154</td>\n",
              "      <td>-0.053513</td>\n",
              "      <td>-0.072334</td>\n",
              "      <td>-0.053850</td>\n",
              "      <td>-0.012665</td>\n",
              "      <td>-0.016878</td>\n",
              "      <td>-0.045585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.034676</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044402</td>\n",
              "      <td>-0.031874</td>\n",
              "      <td>-0.059034</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.007610</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.122053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monthsep</th>\n",
              "      <td>0.076609</td>\n",
              "      <td>0.110907</td>\n",
              "      <td>0.531857</td>\n",
              "      <td>-0.068877</td>\n",
              "      <td>0.088006</td>\n",
              "      <td>-0.062596</td>\n",
              "      <td>-0.181476</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>0.056573</td>\n",
              "      <td>0.107671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.093982</td>\n",
              "      <td>-0.141642</td>\n",
              "      <td>-0.044001</td>\n",
              "      <td>-0.181367</td>\n",
              "      <td>-0.130195</td>\n",
              "      <td>-0.241135</td>\n",
              "      <td>-0.044001</td>\n",
              "      <td>-0.031083</td>\n",
              "      <td>-0.122053</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows Ã— 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-100c48bc-912a-4672-b17d-7c8d7e4e2657')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-100c48bc-912a-4672-b17d-7c8d7e4e2657 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-100c48bc-912a-4672-b17d-7c8d7e4e2657');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KNHbHRq7Kb_",
        "outputId": "6221bbf4-dcd5-40d5-d91d-ff8cc365fb85"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "UkoYrk3i7Ke3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9495594b-d534-4b0d-fe50-8af05e82d0af"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "month            0\n",
              "day              0\n",
              "FFMC             0\n",
              "DMC              0\n",
              "DC               0\n",
              "ISI              0\n",
              "temp             0\n",
              "RH               0\n",
              "wind             0\n",
              "rain             0\n",
              "area             0\n",
              "dayfri           0\n",
              "daymon           0\n",
              "daysat           0\n",
              "daysun           0\n",
              "daythu           0\n",
              "daytue           0\n",
              "daywed           0\n",
              "monthapr         0\n",
              "monthaug         0\n",
              "monthdec         0\n",
              "monthfeb         0\n",
              "monthjan         0\n",
              "monthjul         0\n",
              "monthjun         0\n",
              "monthmar         0\n",
              "monthmay         0\n",
              "monthnov         0\n",
              "monthoct         0\n",
              "monthsep         0\n",
              "size_category    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=data.copy()"
      ],
      "metadata": {
        "id": "2OdoFiSr7Khj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "9wX1C3P_7Klp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "4cdab20e-72f5-4f0e-b50e-51c3a9e78659"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
              "0   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...         0   \n",
              "1   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...         0   \n",
              "\n",
              "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
              "0         0         0         0         1         0         0         0   \n",
              "1         0         0         0         0         0         0         1   \n",
              "\n",
              "   monthsep  size_category  \n",
              "0         0          small  \n",
              "1         0          small  \n",
              "\n",
              "[2 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23930c80-ee86-40ab-b05a-a2a2cb7f149d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>...</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23930c80-ee86-40ab-b05a-a2a2cb7f149d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-23930c80-ee86-40ab-b05a-a2a2cb7f149d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-23930c80-ee86-40ab-b05a-a2a2cb7f149d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(columns=['month','day'], axis=1)"
      ],
      "metadata": {
        "id": "HpZ8PerD7Kn5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "a_nMxVAC8GyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "a55fe53d-9ecb-406c-9de7-cf3403a0b78c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   FFMC   DMC     DC  ISI  temp  RH  wind  rain  area  dayfri  ...  monthfeb  \\\n",
              "0  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0       1  ...         0   \n",
              "1  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0       0  ...         0   \n",
              "\n",
              "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
              "0         0         0         0         1         0         0         0   \n",
              "1         0         0         0         0         0         0         1   \n",
              "\n",
              "   monthsep  size_category  \n",
              "0         0          small  \n",
              "1         0          small  \n",
              "\n",
              "[2 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4e31577-473c-4ce9-a80c-a1be3e2fc03c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>...</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4e31577-473c-4ce9-a80c-a1be3e2fc03c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4e31577-473c-4ce9-a80c-a1be3e2fc03c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4e31577-473c-4ce9-a80c-a1be3e2fc03c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "IK3IOAs98Ibe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220d03f1-58dd-4901-e57e-2e03ddbe295f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area',\n",
              "       'dayfri', 'daymon', 'daysat', 'daysun', 'daythu', 'daytue', 'daywed',\n",
              "       'monthapr', 'monthaug', 'monthdec', 'monthfeb', 'monthjan', 'monthjul',\n",
              "       'monthjun', 'monthmar', 'monthmay', 'monthnov', 'monthoct', 'monthsep',\n",
              "       'size_category'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.size_category.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsPRv3648MCh",
        "outputId": "12993f63-f371-42b3-de51-555b40bcfde2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "small    378\n",
              "large    139\n",
              "Name: size_category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.area.value_counts()"
      ],
      "metadata": {
        "id": "pKPU2baK8MEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8273b75-480e-460c-d700-3f2a02972796"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00      247\n",
              "1.94        3\n",
              "0.52        2\n",
              "3.71        2\n",
              "0.68        2\n",
              "         ... \n",
              "105.66      1\n",
              "154.88      1\n",
              "196.48      1\n",
              "200.94      1\n",
              "11.16       1\n",
              "Name: area, Length: 251, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.rain.value_counts()"
      ],
      "metadata": {
        "id": "H29rMRLB8MHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c664cb15-c373-4b7e-9aa5-4df602d2b069"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    509\n",
              "0.2      2\n",
              "0.8      2\n",
              "1.0      1\n",
              "6.4      1\n",
              "0.4      1\n",
              "1.4      1\n",
              "Name: rain, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df['size_category']= label_encoder.fit_transform(df['size_category'])"
      ],
      "metadata": {
        "id": "FODTo78l8MLA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "ky2EwkMI8MNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22d5622-b655-4068-96a6-d9c4c23c742d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 517 entries, 0 to 516\n",
            "Data columns (total 29 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   FFMC           517 non-null    float64\n",
            " 1   DMC            517 non-null    float64\n",
            " 2   DC             517 non-null    float64\n",
            " 3   ISI            517 non-null    float64\n",
            " 4   temp           517 non-null    float64\n",
            " 5   RH             517 non-null    int64  \n",
            " 6   wind           517 non-null    float64\n",
            " 7   rain           517 non-null    float64\n",
            " 8   area           517 non-null    float64\n",
            " 9   dayfri         517 non-null    int64  \n",
            " 10  daymon         517 non-null    int64  \n",
            " 11  daysat         517 non-null    int64  \n",
            " 12  daysun         517 non-null    int64  \n",
            " 13  daythu         517 non-null    int64  \n",
            " 14  daytue         517 non-null    int64  \n",
            " 15  daywed         517 non-null    int64  \n",
            " 16  monthapr       517 non-null    int64  \n",
            " 17  monthaug       517 non-null    int64  \n",
            " 18  monthdec       517 non-null    int64  \n",
            " 19  monthfeb       517 non-null    int64  \n",
            " 20  monthjan       517 non-null    int64  \n",
            " 21  monthjul       517 non-null    int64  \n",
            " 22  monthjun       517 non-null    int64  \n",
            " 23  monthmar       517 non-null    int64  \n",
            " 24  monthmay       517 non-null    int64  \n",
            " 25  monthnov       517 non-null    int64  \n",
            " 26  monthoct       517 non-null    int64  \n",
            " 27  monthsep       517 non-null    int64  \n",
            " 28  size_category  517 non-null    int64  \n",
            "dtypes: float64(8), int64(21)\n",
            "memory usage: 117.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('size_category', axis=1)\n",
        "y = df['size_category']"
      ],
      "metadata": {
        "id": "K3aO1LUk8MPi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ],
      "metadata": {
        "id": "qj1DVdsB8MSM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(42, input_shape=(28,), activation = 'relu'))\n",
        "model.add(Dense(28, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "j-b7MRtX8MVM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UTInYg0h8MYZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(X_train,y_train, validation_split=0.33, epochs=180, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlBgiZ8i8MZ8",
        "outputId": "6b6357f3-aff9-418b-89db-f5769ba15c9d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/180\n",
            "28/28 [==============================] - 1s 7ms/step - loss: 1.8896 - accuracy: 0.6304 - val_loss: 0.8555 - val_accuracy: 0.6423\n",
            "Epoch 2/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.7790 - val_loss: 0.7101 - val_accuracy: 0.7518\n",
            "Epoch 3/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.8152 - val_loss: 0.4084 - val_accuracy: 0.8175\n",
            "Epoch 4/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.8261 - val_loss: 0.4272 - val_accuracy: 0.8029\n",
            "Epoch 5/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8551 - val_loss: 0.3981 - val_accuracy: 0.8102\n",
            "Epoch 6/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8804 - val_loss: 0.3552 - val_accuracy: 0.8248\n",
            "Epoch 7/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8406 - val_loss: 0.2749 - val_accuracy: 0.8686\n",
            "Epoch 8/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2924 - accuracy: 0.8841 - val_loss: 0.6337 - val_accuracy: 0.5693\n",
            "Epoch 9/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8768 - val_loss: 0.2659 - val_accuracy: 0.8613\n",
            "Epoch 10/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8949 - val_loss: 0.3093 - val_accuracy: 0.9270\n",
            "Epoch 11/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3073 - accuracy: 0.8804 - val_loss: 0.2275 - val_accuracy: 0.8686\n",
            "Epoch 12/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3019 - accuracy: 0.8877 - val_loss: 0.2660 - val_accuracy: 0.8321\n",
            "Epoch 13/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.9058 - val_loss: 0.4374 - val_accuracy: 0.8175\n",
            "Epoch 14/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9167 - val_loss: 0.2054 - val_accuracy: 0.9416\n",
            "Epoch 15/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.9312 - val_loss: 0.5906 - val_accuracy: 0.5985\n",
            "Epoch 16/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.9058 - val_loss: 0.2358 - val_accuracy: 0.9708\n",
            "Epoch 17/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9275 - val_loss: 0.1670 - val_accuracy: 0.9635\n",
            "Epoch 18/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1805 - accuracy: 0.9384 - val_loss: 0.1529 - val_accuracy: 0.9562\n",
            "Epoch 19/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9384 - val_loss: 0.2000 - val_accuracy: 0.9781\n",
            "Epoch 20/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1515 - accuracy: 0.9420 - val_loss: 0.1526 - val_accuracy: 0.9270\n",
            "Epoch 21/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9348 - val_loss: 0.1479 - val_accuracy: 0.9416\n",
            "Epoch 22/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9348 - val_loss: 0.1806 - val_accuracy: 0.8905\n",
            "Epoch 23/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9601 - val_loss: 0.1263 - val_accuracy: 0.9635\n",
            "Epoch 24/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9601 - val_loss: 0.1783 - val_accuracy: 0.8978\n",
            "Epoch 25/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9384 - val_loss: 0.1968 - val_accuracy: 0.8978\n",
            "Epoch 26/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9312 - val_loss: 0.1074 - val_accuracy: 0.9635\n",
            "Epoch 27/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9601 - val_loss: 0.1065 - val_accuracy: 0.9781\n",
            "Epoch 28/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.9529 - val_loss: 0.1238 - val_accuracy: 0.9489\n",
            "Epoch 29/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9493 - val_loss: 0.1112 - val_accuracy: 0.9854\n",
            "Epoch 30/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.9094 - val_loss: 0.9265 - val_accuracy: 0.8175\n",
            "Epoch 31/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9420 - val_loss: 0.1472 - val_accuracy: 0.9708\n",
            "Epoch 32/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9565 - val_loss: 0.0936 - val_accuracy: 0.9562\n",
            "Epoch 33/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9493 - val_loss: 0.1010 - val_accuracy: 0.9562\n",
            "Epoch 34/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9746 - val_loss: 0.0945 - val_accuracy: 0.9635\n",
            "Epoch 35/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.9348 - val_loss: 0.4227 - val_accuracy: 0.8467\n",
            "Epoch 36/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9275 - val_loss: 0.0818 - val_accuracy: 0.9708\n",
            "Epoch 37/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9348 - val_loss: 0.1248 - val_accuracy: 0.9343\n",
            "Epoch 38/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1145 - accuracy: 0.9601 - val_loss: 0.0764 - val_accuracy: 0.9781\n",
            "Epoch 39/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.9674 - val_loss: 0.0781 - val_accuracy: 0.9781\n",
            "Epoch 40/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.9529 - val_loss: 0.0748 - val_accuracy: 0.9708\n",
            "Epoch 41/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0981 - accuracy: 0.9674 - val_loss: 0.1444 - val_accuracy: 0.9343\n",
            "Epoch 42/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9348 - val_loss: 0.1234 - val_accuracy: 0.9635\n",
            "Epoch 43/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.9239 - val_loss: 0.2234 - val_accuracy: 0.8978\n",
            "Epoch 44/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9529 - val_loss: 0.2005 - val_accuracy: 0.9124\n",
            "Epoch 45/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9384 - val_loss: 0.2303 - val_accuracy: 0.9197\n",
            "Epoch 46/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1138 - accuracy: 0.9565 - val_loss: 0.0646 - val_accuracy: 0.9854\n",
            "Epoch 47/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9710 - val_loss: 0.0584 - val_accuracy: 0.9781\n",
            "Epoch 48/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1585 - accuracy: 0.9565 - val_loss: 0.1898 - val_accuracy: 0.9124\n",
            "Epoch 49/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9529 - val_loss: 0.0798 - val_accuracy: 0.9635\n",
            "Epoch 50/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0858 - accuracy: 0.9674 - val_loss: 0.0618 - val_accuracy: 0.9781\n",
            "Epoch 51/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9674 - val_loss: 0.0601 - val_accuracy: 0.9781\n",
            "Epoch 52/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9783 - val_loss: 0.0642 - val_accuracy: 0.9708\n",
            "Epoch 53/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9674 - val_loss: 0.0622 - val_accuracy: 0.9854\n",
            "Epoch 54/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9529 - val_loss: 0.1046 - val_accuracy: 0.9343\n",
            "Epoch 55/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.9638 - val_loss: 0.0637 - val_accuracy: 0.9708\n",
            "Epoch 56/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.9203 - val_loss: 0.0710 - val_accuracy: 0.9708\n",
            "Epoch 57/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.9275 - val_loss: 0.1680 - val_accuracy: 0.9343\n",
            "Epoch 58/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9710 - val_loss: 0.0607 - val_accuracy: 0.9708\n",
            "Epoch 59/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9783 - val_loss: 0.0598 - val_accuracy: 0.9708\n",
            "Epoch 60/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9674 - val_loss: 0.1421 - val_accuracy: 0.9343\n",
            "Epoch 61/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9746 - val_loss: 0.0649 - val_accuracy: 0.9781\n",
            "Epoch 62/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9819 - val_loss: 0.0996 - val_accuracy: 0.9562\n",
            "Epoch 63/180\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9674 - val_loss: 0.0956 - val_accuracy: 0.9489\n",
            "Epoch 64/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2813 - accuracy: 0.9420 - val_loss: 0.0927 - val_accuracy: 0.9489\n",
            "Epoch 65/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9819 - val_loss: 0.0472 - val_accuracy: 0.9781\n",
            "Epoch 66/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9457 - val_loss: 0.1099 - val_accuracy: 0.9489\n",
            "Epoch 67/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.9348 - val_loss: 0.0495 - val_accuracy: 0.9854\n",
            "Epoch 68/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.9239 - val_loss: 0.1015 - val_accuracy: 0.9489\n",
            "Epoch 69/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9783 - val_loss: 0.0502 - val_accuracy: 0.9708\n",
            "Epoch 70/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9674 - val_loss: 0.0781 - val_accuracy: 0.9781\n",
            "Epoch 71/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9710 - val_loss: 0.0531 - val_accuracy: 0.9781\n",
            "Epoch 72/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1061 - accuracy: 0.9565 - val_loss: 0.0598 - val_accuracy: 0.9781\n",
            "Epoch 73/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9746 - val_loss: 0.0423 - val_accuracy: 0.9854\n",
            "Epoch 74/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9601 - val_loss: 0.0480 - val_accuracy: 0.9781\n",
            "Epoch 75/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9638 - val_loss: 0.0664 - val_accuracy: 0.9635\n",
            "Epoch 76/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9746 - val_loss: 0.0431 - val_accuracy: 0.9854\n",
            "Epoch 77/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1185 - accuracy: 0.9565 - val_loss: 0.1835 - val_accuracy: 0.9270\n",
            "Epoch 78/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1045 - accuracy: 0.9601 - val_loss: 0.1121 - val_accuracy: 0.9416\n",
            "Epoch 79/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9384 - val_loss: 0.0420 - val_accuracy: 0.9854\n",
            "Epoch 80/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.9457 - val_loss: 0.0650 - val_accuracy: 0.9781\n",
            "Epoch 81/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9674 - val_loss: 0.2225 - val_accuracy: 0.9197\n",
            "Epoch 82/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9746 - val_loss: 0.0458 - val_accuracy: 0.9781\n",
            "Epoch 83/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9565 - val_loss: 0.1030 - val_accuracy: 0.9562\n",
            "Epoch 84/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9384 - val_loss: 0.1107 - val_accuracy: 0.9416\n",
            "Epoch 85/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9565 - val_loss: 0.0804 - val_accuracy: 0.9635\n",
            "Epoch 86/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9348 - val_loss: 0.0583 - val_accuracy: 0.9708\n",
            "Epoch 87/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9710 - val_loss: 0.1765 - val_accuracy: 0.9416\n",
            "Epoch 88/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9601 - val_loss: 0.0886 - val_accuracy: 0.9562\n",
            "Epoch 89/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9710 - val_loss: 0.0651 - val_accuracy: 0.9708\n",
            "Epoch 90/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9565 - val_loss: 0.2476 - val_accuracy: 0.9197\n",
            "Epoch 91/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9420 - val_loss: 0.1309 - val_accuracy: 0.9416\n",
            "Epoch 92/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9601 - val_loss: 0.0495 - val_accuracy: 0.9708\n",
            "Epoch 93/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9819 - val_loss: 0.0836 - val_accuracy: 0.9635\n",
            "Epoch 94/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9529 - val_loss: 0.3281 - val_accuracy: 0.9051\n",
            "Epoch 95/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.8125 - accuracy: 0.8804 - val_loss: 0.1434 - val_accuracy: 0.9416\n",
            "Epoch 96/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9601 - val_loss: 0.0739 - val_accuracy: 0.9635\n",
            "Epoch 97/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9746 - val_loss: 0.0448 - val_accuracy: 0.9781\n",
            "Epoch 98/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9928 - val_loss: 0.0495 - val_accuracy: 0.9708\n",
            "Epoch 99/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9855 - val_loss: 0.1334 - val_accuracy: 0.9489\n",
            "Epoch 100/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9601 - val_loss: 0.0500 - val_accuracy: 0.9708\n",
            "Epoch 101/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9819 - val_loss: 0.0756 - val_accuracy: 0.9708\n",
            "Epoch 102/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9710 - val_loss: 0.0566 - val_accuracy: 0.9708\n",
            "Epoch 103/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9819 - val_loss: 0.0594 - val_accuracy: 0.9708\n",
            "Epoch 104/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.9819 - val_loss: 0.0488 - val_accuracy: 0.9781\n",
            "Epoch 105/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0412 - accuracy: 0.9855 - val_loss: 0.0645 - val_accuracy: 0.9635\n",
            "Epoch 106/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9529 - val_loss: 0.0612 - val_accuracy: 0.9635\n",
            "Epoch 107/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9674 - val_loss: 0.1013 - val_accuracy: 0.9635\n",
            "Epoch 108/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0562 - val_accuracy: 0.9708\n",
            "Epoch 109/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1782 - accuracy: 0.9384 - val_loss: 0.1867 - val_accuracy: 0.9343\n",
            "Epoch 110/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9638 - val_loss: 0.0398 - val_accuracy: 0.9854\n",
            "Epoch 111/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9928 - val_loss: 0.0434 - val_accuracy: 0.9854\n",
            "Epoch 112/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9855 - val_loss: 0.0555 - val_accuracy: 0.9708\n",
            "Epoch 113/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.9420 - val_loss: 0.0461 - val_accuracy: 0.9781\n",
            "Epoch 114/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9710 - val_loss: 0.0530 - val_accuracy: 0.9781\n",
            "Epoch 115/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9746 - val_loss: 0.0493 - val_accuracy: 0.9781\n",
            "Epoch 116/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1222 - accuracy: 0.9565 - val_loss: 0.1633 - val_accuracy: 0.9416\n",
            "Epoch 117/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9565 - val_loss: 0.1466 - val_accuracy: 0.9343\n",
            "Epoch 118/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9493 - val_loss: 0.0450 - val_accuracy: 0.9854\n",
            "Epoch 119/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9746 - val_loss: 0.0777 - val_accuracy: 0.9708\n",
            "Epoch 120/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2252 - accuracy: 0.9275 - val_loss: 0.6536 - val_accuracy: 0.8759\n",
            "Epoch 121/180\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2158 - accuracy: 0.9565 - val_loss: 0.4138 - val_accuracy: 0.8905\n",
            "Epoch 122/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9601 - val_loss: 0.2325 - val_accuracy: 0.9343\n",
            "Epoch 123/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.9565 - val_loss: 0.0995 - val_accuracy: 0.9635\n",
            "Epoch 124/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.9457 - val_loss: 0.2155 - val_accuracy: 0.9343\n",
            "Epoch 125/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9565 - val_loss: 0.0521 - val_accuracy: 0.9781\n",
            "Epoch 126/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9783 - val_loss: 0.0455 - val_accuracy: 0.9781\n",
            "Epoch 127/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9855 - val_loss: 0.0499 - val_accuracy: 0.9708\n",
            "Epoch 128/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9819 - val_loss: 0.0436 - val_accuracy: 0.9781\n",
            "Epoch 129/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9891 - val_loss: 0.0481 - val_accuracy: 0.9708\n",
            "Epoch 130/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9746 - val_loss: 0.0899 - val_accuracy: 0.9635\n",
            "Epoch 131/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9710 - val_loss: 0.0585 - val_accuracy: 0.9781\n",
            "Epoch 132/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.9638 - val_loss: 0.0530 - val_accuracy: 0.9781\n",
            "Epoch 133/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9819 - val_loss: 0.0549 - val_accuracy: 0.9708\n",
            "Epoch 134/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9819 - val_loss: 0.0571 - val_accuracy: 0.9635\n",
            "Epoch 135/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9601 - val_loss: 0.3340 - val_accuracy: 0.9124\n",
            "Epoch 136/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.9529 - val_loss: 0.1309 - val_accuracy: 0.9489\n",
            "Epoch 137/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9638 - val_loss: 0.0409 - val_accuracy: 0.9854\n",
            "Epoch 138/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9855 - val_loss: 0.0420 - val_accuracy: 0.9781\n",
            "Epoch 139/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.9819 - val_loss: 0.1159 - val_accuracy: 0.9489\n",
            "Epoch 140/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9674 - val_loss: 0.0943 - val_accuracy: 0.9708\n",
            "Epoch 141/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9891 - val_loss: 0.0466 - val_accuracy: 0.9854\n",
            "Epoch 142/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9674 - val_loss: 0.0537 - val_accuracy: 0.9708\n",
            "Epoch 143/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9783 - val_loss: 0.0396 - val_accuracy: 0.9781\n",
            "Epoch 144/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9819 - val_loss: 0.0424 - val_accuracy: 0.9781\n",
            "Epoch 145/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9601 - val_loss: 0.1104 - val_accuracy: 0.9416\n",
            "Epoch 146/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9710 - val_loss: 0.0412 - val_accuracy: 0.9854\n",
            "Epoch 147/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9746 - val_loss: 0.0487 - val_accuracy: 0.9781\n",
            "Epoch 148/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9783 - val_loss: 0.0739 - val_accuracy: 0.9708\n",
            "Epoch 149/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9565 - val_loss: 0.1299 - val_accuracy: 0.9635\n",
            "Epoch 150/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9783 - val_loss: 0.0540 - val_accuracy: 0.9708\n",
            "Epoch 151/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9783 - val_loss: 0.0584 - val_accuracy: 0.9708\n",
            "Epoch 152/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.9891 - val_loss: 0.0971 - val_accuracy: 0.9562\n",
            "Epoch 153/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9783 - val_loss: 0.0390 - val_accuracy: 0.9781\n",
            "Epoch 154/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 0.0939 - val_accuracy: 0.9635\n",
            "Epoch 155/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9783 - val_loss: 0.0991 - val_accuracy: 0.9489\n",
            "Epoch 156/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.9783 - val_loss: 0.0719 - val_accuracy: 0.9708\n",
            "Epoch 157/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9783 - val_loss: 0.0595 - val_accuracy: 0.9635\n",
            "Epoch 158/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9746 - val_loss: 0.3076 - val_accuracy: 0.9270\n",
            "Epoch 159/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9710 - val_loss: 0.1426 - val_accuracy: 0.9416\n",
            "Epoch 160/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9638 - val_loss: 0.0447 - val_accuracy: 0.9781\n",
            "Epoch 161/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9819 - val_loss: 0.0751 - val_accuracy: 0.9635\n",
            "Epoch 162/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 0.0540 - val_accuracy: 0.9708\n",
            "Epoch 163/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9783 - val_loss: 0.1073 - val_accuracy: 0.9635\n",
            "Epoch 164/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9783 - val_loss: 0.1090 - val_accuracy: 0.9635\n",
            "Epoch 165/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0642 - val_accuracy: 0.9708\n",
            "Epoch 166/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9746 - val_loss: 0.0469 - val_accuracy: 0.9708\n",
            "Epoch 167/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9565 - val_loss: 0.0863 - val_accuracy: 0.9635\n",
            "Epoch 168/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9710 - val_loss: 0.0935 - val_accuracy: 0.9635\n",
            "Epoch 169/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9819 - val_loss: 0.0684 - val_accuracy: 0.9708\n",
            "Epoch 170/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9746 - val_loss: 0.4004 - val_accuracy: 0.9197\n",
            "Epoch 171/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9529 - val_loss: 0.0700 - val_accuracy: 0.9562\n",
            "Epoch 172/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9674 - val_loss: 0.0553 - val_accuracy: 0.9708\n",
            "Epoch 173/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9783 - val_loss: 0.0554 - val_accuracy: 0.9708\n",
            "Epoch 174/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9891 - val_loss: 0.0494 - val_accuracy: 0.9708\n",
            "Epoch 175/180\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9891 - val_loss: 0.0681 - val_accuracy: 0.9708\n",
            "Epoch 176/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9819 - val_loss: 0.0685 - val_accuracy: 0.9708\n",
            "Epoch 177/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9710 - val_loss: 0.0528 - val_accuracy: 0.9781\n",
            "Epoch 178/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9674 - val_loss: 0.0442 - val_accuracy: 0.9708\n",
            "Epoch 179/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9891 - val_loss: 0.0651 - val_accuracy: 0.9708\n",
            "Epoch 180/180\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9855 - val_loss: 0.0582 - val_accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_train, y_train)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBSXuFFa8wV8",
        "outputId": "2f9809e1-f4af-4973-b3fc-63c6f6357f4e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9903\n",
            "accuracy: 99.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "JalWHJDw8wYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca3342e-bd22-407c-eb85-85bbe4607f51"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9904\n",
            "accuracy: 99.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "TXbp1FBW8wbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e58604f-8b69-40be-94e0-f63f08e70b49"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer='rmsprop', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "mEInYGNl8wdZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "Xkeyouqn8wfn",
        "outputId": "c7f024df-b978-4f21-c4f2-907b18bc0380"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = df.drop('size_category', axis=1)\n",
        "y1 = df['size_category']"
      ],
      "metadata": {
        "id": "VRU6lvxu8wh1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = StandardScaler()\n",
        "a.fit(X1)\n",
        "X_standardized = a.transform(X1)"
      ],
      "metadata": {
        "id": "v1X8KbNW8wjo"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install StandardScaler"
      ],
      "metadata": {
        "id": "n4dxGN_C8wlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a901d29c-9924-4405-f71d-bf897b044831"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting StandardScaler\n",
            "  Downloading StandardScaler-0.5.tar.gz (2.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from StandardScaler) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from StandardScaler) (1.4.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from StandardScaler) (1.2.2)\n",
            "Collecting scikit-elm\n",
            "  Downloading scikit_elm-0.21a0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.9/dist-packages (from StandardScaler) (2022.12.1)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (2.2.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (8.1.3)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (2023.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (23.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from dask->StandardScaler) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->StandardScaler) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->StandardScaler) (2022.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from scikit-elm->StandardScaler) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->StandardScaler) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->StandardScaler) (3.1.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.9/dist-packages (from partd>=0.3.10->dask->StandardScaler) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->StandardScaler) (1.16.0)\n",
            "Building wheels for collected packages: StandardScaler\n",
            "  Building wheel for StandardScaler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for StandardScaler: filename=StandardScaler-0.5-py3-none-any.whl size=2613 sha256=8429307e700a1a6c5d06abdddb1516c080f75fa46ea6015eabb396321f193134\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/7d/ec/7ea52e96c7cf25f808c75a53cb1db1ef8e7c071debbc983276\n",
            "Successfully built StandardScaler\n",
            "Installing collected packages: scikit-elm, StandardScaler\n",
            "Successfully installed StandardScaler-0.5 scikit-elm-0.21a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = StandardScaler()\n",
        "a.fit(X1)\n",
        "X_standardized = a.transform(X1)"
      ],
      "metadata": {
        "id": "jACDL5b68woS"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "10zC-7XV8wpm"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_standardized).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "han3DMmG8wsC",
        "outputId": "2a42c8c9-9b32-4454-b0a3-39cbd2262fe8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0             1             2             3             4   \\\n",
              "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
              "mean  -1.752306e-15 -2.748715e-17  6.871787e-17  1.030768e-17  2.542561e-16   \n",
              "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
              "min   -1.304582e+01 -1.715608e+00 -2.179108e+00 -1.980578e+00 -2.876943e+00   \n",
              "25%   -8.063453e-02 -6.606652e-01 -4.448281e-01 -5.535954e-01 -5.842379e-01   \n",
              "50%    1.732292e-01 -4.020255e-02  4.691190e-01 -1.364774e-01  7.082076e-02   \n",
              "75%    4.089598e-01  4.927389e-01  6.696628e-01  3.904086e-01  6.741643e-01   \n",
              "max    1.007353e+00  2.819865e+00  1.261610e+00  1.033538e+01  2.484195e+00   \n",
              "\n",
              "                 5             6             7             8             9   \\\n",
              "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
              "mean   2.198972e-16 -4.191790e-16 -6.871787e-18  4.123072e-17  3.435893e-18   \n",
              "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
              "min   -1.796637e+00 -2.021098e+00 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
              "25%   -6.924563e-01 -7.361236e-01 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
              "50%   -1.403660e-01 -9.833712e-03 -7.326831e-02 -1.938429e-01 -4.435755e-01   \n",
              "75%    5.344111e-01  4.929823e-01 -7.326831e-02 -9.870852e-02 -4.435755e-01   \n",
              "max    3.417549e+00  3.007063e+00  2.157228e+01  1.695111e+01  2.254407e+00   \n",
              "\n",
              "       ...            18            19            20            21  \\\n",
              "count  ...  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
              "mean   ... -6.871787e-17  2.061536e-17  2.233331e-17 -1.374357e-17   \n",
              "std    ...  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
              "min    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
              "25%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
              "50%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
              "75%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
              "max    ...  7.512952e+00  4.984977e+00  1.604681e+01  3.893103e+00   \n",
              "\n",
              "                 22            23            24            25            26  \\\n",
              "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
              "mean  -1.374357e-17 -8.246144e-17 -1.717947e-17 -1.030768e-17  5.497429e-17   \n",
              "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
              "min   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
              "25%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
              "50%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
              "75%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
              "max    5.423261e+00  2.928152e+00  1.604681e+01  2.271563e+01  5.785038e+00   \n",
              "\n",
              "                 27  \n",
              "count  5.170000e+02  \n",
              "mean   2.748715e-17  \n",
              "std    1.000969e+00  \n",
              "min   -7.060812e-01  \n",
              "25%   -7.060812e-01  \n",
              "50%   -7.060812e-01  \n",
              "75%    1.416268e+00  \n",
              "max    1.416268e+00  \n",
              "\n",
              "[8 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46aef226-18ac-4cb2-ab66-92a97fb40cde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>5.170000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-1.752306e-15</td>\n",
              "      <td>-2.748715e-17</td>\n",
              "      <td>6.871787e-17</td>\n",
              "      <td>1.030768e-17</td>\n",
              "      <td>2.542561e-16</td>\n",
              "      <td>2.198972e-16</td>\n",
              "      <td>-4.191790e-16</td>\n",
              "      <td>-6.871787e-18</td>\n",
              "      <td>4.123072e-17</td>\n",
              "      <td>3.435893e-18</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.871787e-17</td>\n",
              "      <td>2.061536e-17</td>\n",
              "      <td>2.233331e-17</td>\n",
              "      <td>-1.374357e-17</td>\n",
              "      <td>-1.374357e-17</td>\n",
              "      <td>-8.246144e-17</td>\n",
              "      <td>-1.717947e-17</td>\n",
              "      <td>-1.030768e-17</td>\n",
              "      <td>5.497429e-17</td>\n",
              "      <td>2.748715e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "      <td>1.000969e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.304582e+01</td>\n",
              "      <td>-1.715608e+00</td>\n",
              "      <td>-2.179108e+00</td>\n",
              "      <td>-1.980578e+00</td>\n",
              "      <td>-2.876943e+00</td>\n",
              "      <td>-1.796637e+00</td>\n",
              "      <td>-2.021098e+00</td>\n",
              "      <td>-7.326831e-02</td>\n",
              "      <td>-2.020198e-01</td>\n",
              "      <td>-4.435755e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.331035e-01</td>\n",
              "      <td>-2.006027e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-2.568645e-01</td>\n",
              "      <td>-1.843909e-01</td>\n",
              "      <td>-3.415123e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-4.402255e-02</td>\n",
              "      <td>-1.728597e-01</td>\n",
              "      <td>-7.060812e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-8.063453e-02</td>\n",
              "      <td>-6.606652e-01</td>\n",
              "      <td>-4.448281e-01</td>\n",
              "      <td>-5.535954e-01</td>\n",
              "      <td>-5.842379e-01</td>\n",
              "      <td>-6.924563e-01</td>\n",
              "      <td>-7.361236e-01</td>\n",
              "      <td>-7.326831e-02</td>\n",
              "      <td>-2.020198e-01</td>\n",
              "      <td>-4.435755e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.331035e-01</td>\n",
              "      <td>-2.006027e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-2.568645e-01</td>\n",
              "      <td>-1.843909e-01</td>\n",
              "      <td>-3.415123e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-4.402255e-02</td>\n",
              "      <td>-1.728597e-01</td>\n",
              "      <td>-7.060812e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.732292e-01</td>\n",
              "      <td>-4.020255e-02</td>\n",
              "      <td>4.691190e-01</td>\n",
              "      <td>-1.364774e-01</td>\n",
              "      <td>7.082076e-02</td>\n",
              "      <td>-1.403660e-01</td>\n",
              "      <td>-9.833712e-03</td>\n",
              "      <td>-7.326831e-02</td>\n",
              "      <td>-1.938429e-01</td>\n",
              "      <td>-4.435755e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.331035e-01</td>\n",
              "      <td>-2.006027e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-2.568645e-01</td>\n",
              "      <td>-1.843909e-01</td>\n",
              "      <td>-3.415123e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-4.402255e-02</td>\n",
              "      <td>-1.728597e-01</td>\n",
              "      <td>-7.060812e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.089598e-01</td>\n",
              "      <td>4.927389e-01</td>\n",
              "      <td>6.696628e-01</td>\n",
              "      <td>3.904086e-01</td>\n",
              "      <td>6.741643e-01</td>\n",
              "      <td>5.344111e-01</td>\n",
              "      <td>4.929823e-01</td>\n",
              "      <td>-7.326831e-02</td>\n",
              "      <td>-9.870852e-02</td>\n",
              "      <td>-4.435755e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.331035e-01</td>\n",
              "      <td>-2.006027e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-2.568645e-01</td>\n",
              "      <td>-1.843909e-01</td>\n",
              "      <td>-3.415123e-01</td>\n",
              "      <td>-6.231770e-02</td>\n",
              "      <td>-4.402255e-02</td>\n",
              "      <td>-1.728597e-01</td>\n",
              "      <td>1.416268e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.007353e+00</td>\n",
              "      <td>2.819865e+00</td>\n",
              "      <td>1.261610e+00</td>\n",
              "      <td>1.033538e+01</td>\n",
              "      <td>2.484195e+00</td>\n",
              "      <td>3.417549e+00</td>\n",
              "      <td>3.007063e+00</td>\n",
              "      <td>2.157228e+01</td>\n",
              "      <td>1.695111e+01</td>\n",
              "      <td>2.254407e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>7.512952e+00</td>\n",
              "      <td>4.984977e+00</td>\n",
              "      <td>1.604681e+01</td>\n",
              "      <td>3.893103e+00</td>\n",
              "      <td>5.423261e+00</td>\n",
              "      <td>2.928152e+00</td>\n",
              "      <td>1.604681e+01</td>\n",
              "      <td>2.271563e+01</td>\n",
              "      <td>5.785038e+00</td>\n",
              "      <td>1.416268e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46aef226-18ac-4cb2-ab66-92a97fb40cde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46aef226-18ac-4cb2-ab66-92a97fb40cde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46aef226-18ac-4cb2-ab66-92a97fb40cde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam "
      ],
      "metadata": {
        "id": "Q2nNQQUG8wvL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=28, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    adam=Adam(lr=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "VJWo3mqU90Zn"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "\n",
        "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(X_standardized,y1)"
      ],
      "metadata": {
        "id": "csHhtnRn90dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff43ba3a-2bd4-412a-8aa8-017fb6c9b5ee"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-7c724d92d69e>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[CV 1/5; 1/9] START batch_size=10, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 1/9] END .....batch_size=10, epochs=10;, score=0.933 total time=   1.4s\n",
            "[CV 2/5; 1/9] START batch_size=10, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 1/9] END .....batch_size=10, epochs=10;, score=0.760 total time=   1.3s\n",
            "[CV 3/5; 1/9] START batch_size=10, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 1/9] END .....batch_size=10, epochs=10;, score=0.563 total time=   1.7s\n",
            "[CV 4/5; 1/9] START batch_size=10, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 1/9] END .....batch_size=10, epochs=10;, score=0.689 total time=   1.3s\n",
            "[CV 5/5; 1/9] START batch_size=10, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 1/9] END .....batch_size=10, epochs=10;, score=0.718 total time=   1.4s\n",
            "[CV 1/5; 2/9] START batch_size=10, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 2/9] END .....batch_size=10, epochs=50;, score=0.942 total time=   3.2s\n",
            "[CV 2/5; 2/9] START batch_size=10, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 2/9] END .....batch_size=10, epochs=50;, score=0.808 total time=   6.1s\n",
            "[CV 3/5; 2/9] START batch_size=10, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 2/9] END .....batch_size=10, epochs=50;, score=0.728 total time=   3.3s\n",
            "[CV 4/5; 2/9] START batch_size=10, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 2/9] END .....batch_size=10, epochs=50;, score=0.748 total time=   5.9s\n",
            "[CV 5/5; 2/9] START batch_size=10, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 2/9] END .....batch_size=10, epochs=50;, score=0.845 total time=   3.0s\n",
            "[CV 1/5; 3/9] START batch_size=10, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 3/9] END ....batch_size=10, epochs=100;, score=0.971 total time=   5.5s\n",
            "[CV 2/5; 3/9] START batch_size=10, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 3/9] END ....batch_size=10, epochs=100;, score=0.865 total time=   6.4s\n",
            "[CV 3/5; 3/9] START batch_size=10, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 3/9] END ....batch_size=10, epochs=100;, score=0.854 total time=   5.4s\n",
            "[CV 4/5; 3/9] START batch_size=10, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 3/9] END ....batch_size=10, epochs=100;, score=0.903 total time=   6.1s\n",
            "[CV 5/5; 3/9] START batch_size=10, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 3/9] END ....batch_size=10, epochs=100;, score=0.883 total time=   6.0s\n",
            "[CV 1/5; 4/9] START batch_size=20, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 4/9] END .....batch_size=20, epochs=10;, score=0.990 total time=   1.6s\n",
            "[CV 2/5; 4/9] START batch_size=20, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 4/9] END .....batch_size=20, epochs=10;, score=0.731 total time=   1.1s\n",
            "[CV 3/5; 4/9] START batch_size=20, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 4/9] END .....batch_size=20, epochs=10;, score=0.563 total time=   1.0s\n",
            "[CV 4/5; 4/9] START batch_size=20, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 4/9] END .....batch_size=20, epochs=10;, score=0.670 total time=   1.1s\n",
            "[CV 5/5; 4/9] START batch_size=20, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 4/9] END .....batch_size=20, epochs=10;, score=0.728 total time=   1.1s\n",
            "[CV 1/5; 5/9] START batch_size=20, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 5/9] END .....batch_size=20, epochs=50;, score=0.923 total time=   1.9s\n",
            "[CV 2/5; 5/9] START batch_size=20, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 5/9] END .....batch_size=20, epochs=50;, score=0.769 total time=   2.1s\n",
            "[CV 3/5; 5/9] START batch_size=20, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 5/9] END .....batch_size=20, epochs=50;, score=0.699 total time=   2.5s\n",
            "[CV 4/5; 5/9] START batch_size=20, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 5/9] END .....batch_size=20, epochs=50;, score=0.748 total time=   2.1s\n",
            "[CV 5/5; 5/9] START batch_size=20, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 5/9] END .....batch_size=20, epochs=50;, score=0.718 total time=   2.1s\n",
            "[CV 1/5; 6/9] START batch_size=20, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 6/9] END ....batch_size=20, epochs=100;, score=0.923 total time=   3.3s\n",
            "[CV 2/5; 6/9] START batch_size=20, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 6/9] END ....batch_size=20, epochs=100;, score=0.808 total time=   3.4s\n",
            "[CV 3/5; 6/9] START batch_size=20, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 6/9] END ....batch_size=20, epochs=100;, score=0.786 total time=   3.4s\n",
            "[CV 4/5; 6/9] START batch_size=20, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 6/9] END ....batch_size=20, epochs=100;, score=0.854 total time=   3.3s\n",
            "[CV 5/5; 6/9] START batch_size=20, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 6/9] END ....batch_size=20, epochs=100;, score=0.825 total time=   3.6s\n",
            "[CV 1/5; 7/9] START batch_size=40, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 7/9] END .....batch_size=40, epochs=10;, score=0.923 total time=   1.2s\n",
            "[CV 2/5; 7/9] START batch_size=40, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 7/9] END .....batch_size=40, epochs=10;, score=0.702 total time=   1.3s\n",
            "[CV 3/5; 7/9] START batch_size=40, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 7/9] END .....batch_size=40, epochs=10;, score=0.563 total time=   1.0s\n",
            "[CV 4/5; 7/9] START batch_size=40, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb582c4ddc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 7/9] END .....batch_size=40, epochs=10;, score=0.621 total time=   1.0s\n",
            "[CV 5/5; 7/9] START batch_size=40, epochs=10....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb582ff88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 7/9] END .....batch_size=40, epochs=10;, score=0.689 total time=   0.9s\n",
            "[CV 1/5; 8/9] START batch_size=40, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 8/9] END .....batch_size=40, epochs=50;, score=0.962 total time=   1.5s\n",
            "[CV 2/5; 8/9] START batch_size=40, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 8/9] END .....batch_size=40, epochs=50;, score=0.740 total time=   2.1s\n",
            "[CV 3/5; 8/9] START batch_size=40, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 8/9] END .....batch_size=40, epochs=50;, score=0.660 total time=   2.1s\n",
            "[CV 4/5; 8/9] START batch_size=40, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 8/9] END .....batch_size=40, epochs=50;, score=0.689 total time=   2.1s\n",
            "[CV 5/5; 8/9] START batch_size=40, epochs=50....................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 8/9] END .....batch_size=40, epochs=50;, score=0.738 total time=   2.2s\n",
            "[CV 1/5; 9/9] START batch_size=40, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 9/9] END ....batch_size=40, epochs=100;, score=0.913 total time=   3.3s\n",
            "[CV 2/5; 9/9] START batch_size=40, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 9/9] END ....batch_size=40, epochs=100;, score=0.779 total time=   3.3s\n",
            "[CV 3/5; 9/9] START batch_size=40, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 9/9] END ....batch_size=40, epochs=100;, score=0.660 total time=   2.3s\n",
            "[CV 4/5; 9/9] START batch_size=40, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 9/9] END ....batch_size=40, epochs=100;, score=0.748 total time=   2.5s\n",
            "[CV 5/5; 9/9] START batch_size=40, epochs=100...................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 9/9] END ....batch_size=40, epochs=100;, score=0.748 total time=   2.7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ],
      "metadata": {
        "id": "8oIQ_VcX90jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210c74fd-497e-4ebe-afa9-be499cd30070"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best : 0.8954630374908448, using {'batch_size': 10, 'epochs': 100}\n",
            "0.732636284828186,0.1196395788057789 with: {'batch_size': 10, 'epochs': 10}\n",
            "0.8140776753425598,0.07645201441898443 with: {'batch_size': 10, 'epochs': 50}\n",
            "0.8954630374908448,0.041282593400144646 with: {'batch_size': 10, 'epochs': 100}\n",
            "0.7364637732505799,0.1407552911943509 with: {'batch_size': 20, 'epochs': 10}\n",
            "0.7714712619781494,0.07952346272796094 with: {'batch_size': 20, 'epochs': 50}\n",
            "0.8393577218055726,0.047412809848247306 with: {'batch_size': 20, 'epochs': 100}\n",
            "0.6997572779655457,0.12231508433715084 with: {'batch_size': 40, 'epochs': 10}\n",
            "0.7578603386878967,0.10623222207422443 with: {'batch_size': 40, 'epochs': 50}\n",
            "0.7695295095443726,0.08212386531770278 with: {'batch_size': 40, 'epochs': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "def create_model(learning_rate,dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(4,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam = Adam(lr = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
        "\n",
        "\n",
        "learning_rate = [0.001,0.01,0.1]\n",
        "dropout_rate = [0.0,0.1,0.2]\n",
        "\n",
        "\n",
        "param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(X_standardized,y1)"
      ],
      "metadata": {
        "id": "dZkGn67l90nI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87949b37-b8c9-413f-cf09-45289018e7a5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-c6c3cbca7b83>:17: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[CV 1/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.524 total time=   1.0s\n",
            "[CV 4/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.680 total time=   1.0s\n",
            "[CV 5/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.689 total time=   1.0s\n",
            "[CV 1/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=1.000 total time=   1.9s\n",
            "[CV 2/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.524 total time=   1.1s\n",
            "[CV 4/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.680 total time=   1.0s\n",
            "[CV 5/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=1.000 total time=   1.3s\n",
            "[CV 2/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.750 total time=   1.3s\n",
            "[CV 3/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.524 total time=   1.0s\n",
            "[CV 4/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=1.000 total time=   1.2s\n",
            "[CV 2/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.524 total time=   1.1s\n",
            "[CV 4/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.680 total time=   1.1s\n",
            "[CV 5/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=1.000 total time=   1.3s\n",
            "[CV 2/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.680 total time=   1.2s\n",
            "[CV 5/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=1.000 total time=   1.0s\n",
            "[CV 2/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.680 total time=   1.2s\n",
            "[CV 5/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.718 total time=   1.1s\n",
            "[CV 1/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=1.000 total time=   3.1s\n",
            "[CV 2/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.750 total time=   2.5s\n",
            "[CV 3/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.524 total time=   2.0s\n",
            "[CV 4/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.680 total time=   1.0s\n",
            "[CV 5/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.699 total time=   1.2s\n",
            "[CV 1/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=1.000 total time=   1.0s\n",
            "[CV 2/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.524 total time=   2.8s\n",
            "[CV 4/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.670 total time=   1.2s\n",
            "[CV 5/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=1.000 total time=   2.5s\n",
            "[CV 2/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.699 total time=   1.7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ],
      "metadata": {
        "id": "2pCGOHFuAvOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7fd875-16e4-4d93-9807-5e67f1f77d42"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best : 0.734466016292572, using {'dropout_rate': 0.1, 'learning_rate': 0.1}\n",
            "0.7286407709121704,0.1547957721980402 with: {'dropout_rate': 0.0, 'learning_rate': 0.001}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.0, 'learning_rate': 0.01}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.0, 'learning_rate': 0.1}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.1, 'learning_rate': 0.001}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
            "0.734466016292572,0.1537509780062806 with: {'dropout_rate': 0.1, 'learning_rate': 0.1}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.2, 'learning_rate': 0.001}\n",
            "0.7286407828330994,0.15503914905677466 with: {'dropout_rate': 0.2, 'learning_rate': 0.01}\n",
            "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.2, 'learning_rate': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(activation_function,init):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(4,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam = Adam(lr = 0.001)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
        "\n",
        "# Define the grid search parameters\n",
        "activation_function = ['softmax','relu','tanh','linear']\n",
        "init = ['uniform','normal','zero']\n",
        "\n",
        "# Make a dictionary of the grid search parameters\n",
        "param_grids = dict(activation_function = activation_function,init = init)\n",
        "\n",
        "# Build and fit the GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(X_standardized,y)"
      ],
      "metadata": {
        "id": "JQjxP5XYAvQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695a5843-bedc-46c7-9be9-c995fc291bf4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-522f0a6b0d1b>:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV 1/5; 1/12] START activation_function=softmax, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 1/12] END activation_function=softmax, init=uniform;, score=1.000 total time=   1.9s\n",
            "[CV 2/5; 1/12] START activation_function=softmax, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 1/12] END activation_function=softmax, init=uniform;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 1/12] START activation_function=softmax, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 1/12] END activation_function=softmax, init=uniform;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 1/12] START activation_function=softmax, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 1/12] END activation_function=softmax, init=uniform;, score=0.680 total time=   2.1s\n",
            "[CV 5/5; 1/12] START activation_function=softmax, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 1/12] END activation_function=softmax, init=uniform;, score=0.699 total time=   1.7s\n",
            "[CV 1/5; 2/12] START activation_function=softmax, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 2/12] END activation_function=softmax, init=normal;, score=1.000 total time=   1.1s\n",
            "[CV 2/5; 2/12] START activation_function=softmax, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 2/12] END activation_function=softmax, init=normal;, score=0.750 total time=   2.2s\n",
            "[CV 3/5; 2/12] START activation_function=softmax, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 2/12] END activation_function=softmax, init=normal;, score=0.524 total time=   1.9s\n",
            "[CV 4/5; 2/12] START activation_function=softmax, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 2/12] END activation_function=softmax, init=normal;, score=0.680 total time=   1.7s\n",
            "[CV 5/5; 2/12] START activation_function=softmax, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 2/12] END activation_function=softmax, init=normal;, score=0.699 total time=   2.0s\n",
            "[CV 1/5; 3/12] START activation_function=softmax, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 3/12] END activation_function=softmax, init=zero;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 3/12] START activation_function=softmax, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 3/12] END activation_function=softmax, init=zero;, score=0.250 total time=   1.5s\n",
            "[CV 3/5; 3/12] START activation_function=softmax, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 3/12] END activation_function=softmax, init=zero;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 3/12] START activation_function=softmax, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 3/12] END activation_function=softmax, init=zero;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 3/12] START activation_function=softmax, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 3/12] END activation_function=softmax, init=zero;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 4/12] START activation_function=relu, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 4/12] END activation_function=relu, init=uniform;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 4/12] START activation_function=relu, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 4/12] END activation_function=relu, init=uniform;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 4/12] START activation_function=relu, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 4/12] END activation_function=relu, init=uniform;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 4/12] START activation_function=relu, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 4/12] END activation_function=relu, init=uniform;, score=0.680 total time=   1.0s\n",
            "[CV 5/5; 4/12] START activation_function=relu, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 4/12] END activation_function=relu, init=uniform;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 5/12] START activation_function=relu, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 5/12] END activation_function=relu, init=normal;, score=1.000 total time=   1.2s\n",
            "[CV 2/5; 5/12] START activation_function=relu, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 5/12] END activation_function=relu, init=normal;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 5/12] START activation_function=relu, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 5/12] END activation_function=relu, init=normal;, score=0.524 total time=   1.7s\n",
            "[CV 4/5; 5/12] START activation_function=relu, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 5/12] END activation_function=relu, init=normal;, score=0.680 total time=   1.1s\n",
            "[CV 5/5; 5/12] START activation_function=relu, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 5/12] END activation_function=relu, init=normal;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 6/12] START activation_function=relu, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 6/12] END activation_function=relu, init=zero;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 6/12] START activation_function=relu, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 6/12] END activation_function=relu, init=zero;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 6/12] START activation_function=relu, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 6/12] END activation_function=relu, init=zero;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 6/12] START activation_function=relu, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 6/12] END activation_function=relu, init=zero;, score=0.680 total time=   1.2s\n",
            "[CV 5/5; 6/12] START activation_function=relu, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 6/12] END activation_function=relu, init=zero;, score=0.699 total time=   2.0s\n",
            "[CV 1/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 7/12] END activation_function=tanh, init=uniform;, score=0.971 total time=   1.5s\n",
            "[CV 2/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 7/12] END activation_function=tanh, init=uniform;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 7/12] END activation_function=tanh, init=uniform;, score=0.612 total time=   1.5s\n",
            "[CV 4/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 7/12] END activation_function=tanh, init=uniform;, score=0.680 total time=   1.2s\n",
            "[CV 5/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 7/12] END activation_function=tanh, init=uniform;, score=0.709 total time=   1.1s\n",
            "[CV 1/5; 8/12] START activation_function=tanh, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 8/12] END activation_function=tanh, init=normal;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 8/12] START activation_function=tanh, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 8/12] END activation_function=tanh, init=normal;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 8/12] START activation_function=tanh, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 8/12] END activation_function=tanh, init=normal;, score=0.602 total time=   1.3s\n",
            "[CV 4/5; 8/12] START activation_function=tanh, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 8/12] END activation_function=tanh, init=normal;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 8/12] START activation_function=tanh, init=normal......................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 8/12] END activation_function=tanh, init=normal;, score=0.728 total time=   1.0s\n",
            "[CV 1/5; 9/12] START activation_function=tanh, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 9/12] END activation_function=tanh, init=zero;, score=1.000 total time=   1.0s\n",
            "[CV 2/5; 9/12] START activation_function=tanh, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 9/12] END activation_function=tanh, init=zero;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 9/12] START activation_function=tanh, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 9/12] END activation_function=tanh, init=zero;, score=0.524 total time=   1.0s\n",
            "[CV 4/5; 9/12] START activation_function=tanh, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 9/12] END activation_function=tanh, init=zero;, score=0.680 total time=   1.1s\n",
            "[CV 5/5; 9/12] START activation_function=tanh, init=zero........................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 9/12] END activation_function=tanh, init=zero;, score=0.699 total time=   1.0s\n",
            "[CV 1/5; 10/12] START activation_function=linear, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 10/12] END activation_function=linear, init=uniform;, score=0.942 total time=   1.0s\n",
            "[CV 2/5; 10/12] START activation_function=linear, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 10/12] END activation_function=linear, init=uniform;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 10/12] START activation_function=linear, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 10/12] END activation_function=linear, init=uniform;, score=0.631 total time=   1.0s\n",
            "[CV 4/5; 10/12] START activation_function=linear, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 10/12] END activation_function=linear, init=uniform;, score=0.680 total time=   1.3s\n",
            "[CV 5/5; 10/12] START activation_function=linear, init=uniform..................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 10/12] END activation_function=linear, init=uniform;, score=0.689 total time=   1.7s\n",
            "[CV 1/5; 11/12] START activation_function=linear, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 11/12] END activation_function=linear, init=normal;, score=0.933 total time=   1.1s\n",
            "[CV 2/5; 11/12] START activation_function=linear, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 11/12] END activation_function=linear, init=normal;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 11/12] START activation_function=linear, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 11/12] END activation_function=linear, init=normal;, score=0.573 total time=   1.1s\n",
            "[CV 4/5; 11/12] START activation_function=linear, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 11/12] END activation_function=linear, init=normal;, score=0.670 total time=   1.0s\n",
            "[CV 5/5; 11/12] START activation_function=linear, init=normal...................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 11/12] END activation_function=linear, init=normal;, score=0.709 total time=   1.2s\n",
            "[CV 1/5; 12/12] START activation_function=linear, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 12/12] END activation_function=linear, init=zero;, score=1.000 total time=   1.2s\n",
            "[CV 2/5; 12/12] START activation_function=linear, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 12/12] END activation_function=linear, init=zero;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 12/12] START activation_function=linear, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 12/12] END activation_function=linear, init=zero;, score=0.524 total time=   1.2s\n",
            "[CV 4/5; 12/12] START activation_function=linear, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 12/12] END activation_function=linear, init=zero;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 12/12] START activation_function=linear, init=zero.....................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 12/12] END activation_function=linear, init=zero;, score=0.699 total time=   1.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ],
      "metadata": {
        "id": "SvWwal1WAvTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496ebc36-7c13-4047-f06c-dc95ebce70c2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best : 0.7519417405128479, using {'activation_function': 'tanh', 'init': 'normal'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'uniform'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'normal'}\n",
            "0.6305825233459472,0.24482772813004766 with: {'activation_function': 'softmax', 'init': 'zero'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'uniform'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'normal'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'zero'}\n",
            "0.7442307591438293,0.12209855765840223 with: {'activation_function': 'tanh', 'init': 'uniform'}\n",
            "0.7519417405128479,0.13404036944421382 with: {'activation_function': 'tanh', 'init': 'normal'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'tanh', 'init': 'zero'}\n",
            "0.7384615302085876,0.10871793801195434 with: {'activation_function': 'linear', 'init': 'uniform'}\n",
            "0.726829719543457,0.11849439142048006 with: {'activation_function': 'linear', 'init': 'normal'}\n",
            "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'linear', 'init': 'zero'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(neuron1,neuron2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam = Adam(lr = 0.001)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
        "\n",
        "neuron1 = [4,8,16]\n",
        "neuron2 = [2,4,8]\n",
        "\n",
        "param_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(X_standardized,y)"
      ],
      "metadata": {
        "id": "uRQZ20ToAvWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dac4687-324c-4879-8bdc-1d30c320c1b6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-10070a55e0f3>:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[CV 1/5; 1/9] START neuron1=4, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.981 total time=   1.0s\n",
            "[CV 2/5; 1/9] START neuron1=4, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 1/9] START neuron1=4, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.592 total time=   1.2s\n",
            "[CV 4/5; 1/9] START neuron1=4, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.680 total time=   1.8s\n",
            "[CV 5/5; 1/9] START neuron1=4, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.709 total time=   1.1s\n",
            "[CV 1/5; 2/9] START neuron1=4, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.933 total time=   1.0s\n",
            "[CV 2/5; 2/9] START neuron1=4, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 2/9] START neuron1=4, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.583 total time=   1.5s\n",
            "[CV 4/5; 2/9] START neuron1=4, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.680 total time=   1.1s\n",
            "[CV 5/5; 2/9] START neuron1=4, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.709 total time=   1.0s\n",
            "[CV 1/5; 3/9] START neuron1=4, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 3/9] END .........neuron1=4, neuron2=8;, score=1.000 total time=   1.2s\n",
            "[CV 2/5; 3/9] START neuron1=4, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 3/9] START neuron1=4, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.631 total time=   1.0s\n",
            "[CV 4/5; 3/9] START neuron1=4, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.680 total time=   1.0s\n",
            "[CV 5/5; 3/9] START neuron1=4, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.709 total time=   1.2s\n",
            "[CV 1/5; 4/9] START neuron1=8, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.952 total time=   1.0s\n",
            "[CV 2/5; 4/9] START neuron1=8, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.750 total time=   1.0s\n",
            "[CV 3/5; 4/9] START neuron1=8, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.621 total time=   2.1s\n",
            "[CV 4/5; 4/9] START neuron1=8, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.680 total time=   1.3s\n",
            "[CV 5/5; 4/9] START neuron1=8, neuron2=2........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.680 total time=   1.1s\n",
            "[CV 1/5; 5/9] START neuron1=8, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.933 total time=   1.0s\n",
            "[CV 2/5; 5/9] START neuron1=8, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.750 total time=   1.2s\n",
            "[CV 3/5; 5/9] START neuron1=8, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.602 total time=   1.0s\n",
            "[CV 4/5; 5/9] START neuron1=8, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.680 total time=   1.2s\n",
            "[CV 5/5; 5/9] START neuron1=8, neuron2=4........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.709 total time=   1.0s\n",
            "[CV 1/5; 6/9] START neuron1=8, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.942 total time=   1.2s\n",
            "[CV 2/5; 6/9] START neuron1=8, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 6/9] START neuron1=8, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.621 total time=   1.4s\n",
            "[CV 4/5; 6/9] START neuron1=8, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.689 total time=   1.0s\n",
            "[CV 5/5; 6/9] START neuron1=8, neuron2=8........................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.709 total time=   1.0s\n",
            "[CV 1/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.942 total time=   1.0s\n",
            "[CV 2/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.750 total time=   1.8s\n",
            "[CV 3/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.621 total time=   1.1s\n",
            "[CV 4/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 5/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.709 total time=   1.1s\n",
            "[CV 1/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.962 total time=   1.1s\n",
            "[CV 2/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.750 total time=   1.3s\n",
            "[CV 3/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.650 total time=   1.3s\n",
            "[CV 4/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.699 total time=   1.2s\n",
            "[CV 5/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.680 total time=   1.0s\n",
            "[CV 1/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.962 total time=   1.0s\n",
            "[CV 2/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.750 total time=   1.1s\n",
            "[CV 3/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.650 total time=   1.0s\n",
            "[CV 4/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.699 total time=   1.2s\n",
            "[CV 5/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.718 total time=   1.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtb0Tjk8AvZW",
        "outputId": "9e2db828-478b-489c-cd35-8a788334dac3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best : 0.7558999300003052, using {'neuron1': 16, 'neuron2': 8}\n",
            "0.7422703385353089,0.13000246718595412 with: {'neuron1': 4, 'neuron2': 2}\n",
            "0.7307132124900818,0.11508411752025235 with: {'neuron1': 4, 'neuron2': 4}\n",
            "0.7538834810256958,0.1290166798056773 with: {'neuron1': 4, 'neuron2': 8}\n",
            "0.7365011096000671,0.11516885171409466 with: {'neuron1': 8, 'neuron2': 2}\n",
            "0.7345967054367065,0.11024381606011092 with: {'neuron1': 8, 'neuron2': 4}\n",
            "0.7423450350761414,0.10827953795955239 with: {'neuron1': 8, 'neuron2': 8}\n",
            "0.7442867875099182,0.10739468546390292 with: {'neuron1': 16, 'neuron2': 2}\n",
            "0.7481329321861268,0.11152289020450083 with: {'neuron1': 16, 'neuron2': 4}\n",
            "0.7558999300003052,0.10776943141198134 with: {'neuron1': 16, 'neuron2': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam = Adam(lr = 0.001) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
        "\n",
        "model.fit(X_standardized,y1)\n",
        "\n",
        "y_predict = model.predict(X_standardized)\n",
        "\n",
        "print(accuracy_score(y1,y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbLivTJmAvcA",
        "outputId": "f0d0cf0a-3ae6-4436-afd4-938e1cb6e04b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-0500381b7ebd>:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step\n",
            "0.7775628626692457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam = Adam(lr = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "learning_rate = [0.001,0.01,0.1]\n",
        "dropout_rate = [0.0,0.1,0.2]\n",
        "activation_function = ['softmax','relu','tanh','linear']\n",
        "init = ['uniform','normal','zero']\n",
        "neuron1 = [4,8,16]\n",
        "neuron2 = [2,4,8]\n",
        "\n",
        "# Make a dictionary of the grid search parameters\n",
        "\n",
        "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
        "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n",
        "\n",
        "# Build and fit the GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(X_standardized,y1)\n",
        "\n",
        "# Summarize the results\n",
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ],
      "metadata": {
        "id": "wgoswynXAvep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e16dfc-44cd-4a5d-a417-51e4de2db7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-0620f9c60ee8>:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n",
            "[CV 1/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.9s\n",
            "[CV 4/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   2.4s\n",
            "[CV 1/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.320 total time=   1.5s\n",
            "[CV 5/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   2.3s\n",
            "[CV 3/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   2.2s\n",
            "[CV 4/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   1.9s\n",
            "[CV 5/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.699 total time=   2.3s\n",
            "[CV 1/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=1.000 total time=   2.3s\n",
            "[CV 2/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.699 total time=   1.3s\n",
            "[CV 1/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=1.000 total time=   1.3s\n",
            "[CV 2/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   1.3s\n",
            "[CV 3/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
            "[CV 3/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=1.000 total time=   2.4s\n",
            "[CV 2/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=1.000 total time=   1.3s\n",
            "[CV 2/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.750 total time=   1.3s\n",
            "[CV 3/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.524 total time=   2.3s\n",
            "[CV 4/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.680 total time=   2.2s\n",
            "[CV 5/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.699 total time=   1.8s\n",
            "[CV 1/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
            "[CV 3/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.524 total time=   2.3s\n",
            "[CV 4/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.680 total time=   2.1s\n",
            "[CV 5/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.680 total time=   2.2s\n",
            "[CV 5/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.750 total time=   2.4s\n",
            "[CV 3/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.524 total time=   2.1s\n",
            "[CV 4/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.524 total time=   2.2s\n",
            "[CV 4/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=1.000 total time=   1.8s\n",
            "[CV 2/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.524 total time=   2.2s\n",
            "[CV 4/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.524 total time=   1.9s\n",
            "[CV 4/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=1.000 total time=   2.4s\n",
            "[CV 2/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.524 total time=   2.2s\n",
            "[CV 4/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.680 total time=   1.7s\n",
            "[CV 5/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.524 total time=   1.8s\n",
            "[CV 4/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.621 total time=   1.5s\n",
            "[CV 1/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.680 total time=   1.3s\n",
            "[CV 5/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.699 total time=   2.2s\n",
            "[CV 1/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.000 total time=   1.5s\n",
            "[CV 2/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   2.3s\n",
            "[CV 3/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   2.2s\n",
            "[CV 5/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.3s\n",
            "[CV 4/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   1.7s\n",
            "[CV 1/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=1.000 total time=   2.4s\n",
            "[CV 2/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.250 total time=   1.6s\n",
            "[CV 3/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   2.3s\n",
            "[CV 4/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   2.3s\n",
            "[CV 5/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.699 total time=   2.1s\n",
            "[CV 1/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   2.3s\n",
            "[CV 3/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.699 total time=   2.6s\n",
            "[CV 1/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.000 total time=   1.6s\n",
            "[CV 2/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.437 total time=   1.5s\n",
            "[CV 5/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.680 total time=   2.2s\n",
            "[CV 5/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.699 total time=   2.3s\n",
            "[CV 1/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=1.000 total time=   1.9s\n",
            "[CV 2/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.680 total time=   1.8s\n",
            "[CV 5/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.699 total time=   2.1s\n",
            "[CV 1/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4;, score=1.000 total time=   2.4s\n",
            "[CV 2/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.699 total time=   1.3s\n",
            "[CV 1/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.524 total time=   2.3s\n",
            "[CV 4/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2;, score=1.000 total time=   2.5s\n",
            "[CV 2/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.680 total time=   1.8s\n",
            "[CV 5/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.750 total time=   1.8s\n",
            "[CV 3/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2;, score=1.000 total time=   2.5s\n",
            "[CV 2/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.680 total time=   2.0s\n",
            "[CV 5/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.301 total time=   1.5s\n",
            "[CV 1/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.750 total time=   2.3s\n",
            "[CV 3/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2;, score=1.000 total time=   2.1s\n",
            "[CV 2/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.750 total time=   1.8s\n",
            "[CV 3/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.524 total time=   2.5s\n",
            "[CV 4/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.699 total time=   2.3s\n",
            "[CV 1/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   1.3s\n",
            "[CV 2/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.250 total time=   2.4s\n",
            "[CV 3/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.476 total time=   1.9s\n",
            "[CV 4/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.7s\n",
            "[CV 5/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   2.4s\n",
            "[CV 1/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   2.5s\n",
            "[CV 4/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.680 total time=   1.7s\n",
            "[CV 5/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.250 total time=   1.5s\n",
            "[CV 3/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   1.8s\n",
            "[CV 1/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   2.4s\n",
            "[CV 4/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.699 total time=   2.3s\n",
            "[CV 1/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.699 total time=   2.4s\n",
            "[CV 1/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4;, score=1.000 total time=   1.8s\n",
            "[CV 2/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.750 total time=   1.9s\n",
            "[CV 3/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.524 total time=   1.4s\n",
            "[CV 4/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.680 total time=   2.3s\n",
            "[CV 5/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.524 total time=   1.3s\n",
            "[CV 4/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.699 total time=   1.3s\n",
            "[CV 1/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4;, score=1.000 total time=   3.3s\n",
            "[CV 2/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.750 total time=   1.8s\n",
            "[CV 3/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.524 total time=   2.4s\n",
            "[CV 4/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.250 total time=   1.5s\n",
            "[CV 3/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.699 total time=   1.8s\n",
            "[CV 1/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.750 total time=   2.6s\n",
            "[CV 3/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.524 total time=   1.7s\n",
            "[CV 4/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.750 total time=   2.5s\n",
            "[CV 3/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.680 total time=   1.6s\n",
            "[CV 5/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.000 total time=   1.6s\n",
            "[CV 2/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.680 total time=   2.4s\n",
            "[CV 5/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.750 total time=   1.8s\n",
            "[CV 3/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.524 total time=   3.2s\n",
            "[CV 4/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.680 total time=   2.7s\n",
            "[CV 5/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.699 total time=   1.7s\n",
            "[CV 1/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.699 total time=   1.5s\n",
            "[CV 1/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2;, score=1.000 total time=   2.3s\n",
            "[CV 2/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4;, score=1.000 total time=   1.5s\n",
            "[CV 2/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.750 total time=   1.5s\n",
            "[CV 3/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.680 total time=   2.8s\n",
            "[CV 5/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.699 total time=   1.8s\n",
            "[CV 1/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.750 total time=   1.6s\n",
            "[CV 3/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.699 total time=   2.3s\n",
            "[CV 1/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2;, score=1.000 total time=   1.6s\n",
            "[CV 2/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.750 total time=   1.4s\n",
            "[CV 3/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.524 total time=   1.5s\n",
            "[CV 4/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.680 total time=   1.4s\n",
            "[CV 5/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.699 total time=   1.4s\n",
            "[CV 1/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4;, score=1.000 total time=   1.4s\n",
            "[CV 2/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.750 total time=   2.2s\n",
            "[CV 3/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.524 total time=   1.6s\n",
            "[CV 4/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.680 total time=   1.5s\n",
            "[CV 5/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.699 total time=   2.6s\n",
            "[CV 1/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8;, score=1.000 total time=   1.7s\n",
            "[CV 2/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.750 total time=   1.7s\n",
            "[CV 3/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.524 total time=   1.7s\n",
            "[CV 4/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.680 total time=   2.5s\n",
            "[CV 5/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.699 total time=   1.6s\n",
            "[CV 1/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.990 total time=   3.6s\n",
            "[CV 2/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   3.5s\n",
            "[CV 3/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   6.2s\n",
            "[CV 4/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.738 total time=   3.5s\n",
            "[CV 5/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.777 total time=   3.7s\n",
            "[CV 1/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   3.5s\n",
            "[CV 2/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.767 total time=   6.2s\n",
            "[CV 5/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.767 total time=   3.4s\n",
            "[CV 1/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.990 total time=   5.2s\n",
            "[CV 2/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.760 total time=   6.2s\n",
            "[CV 3/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   4.0s\n",
            "[CV 4/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.738 total time=   3.9s\n",
            "[CV 5/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.767 total time=   3.5s\n",
            "[CV 1/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.981 total time=   3.7s\n",
            "[CV 2/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   4.0s\n",
            "[CV 3/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   6.0s\n",
            "[CV 4/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.728 total time=   6.0s\n",
            "[CV 5/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.738 total time=   3.5s\n",
            "[CV 1/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.981 total time=   3.5s\n",
            "[CV 2/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   6.1s\n",
            "[CV 3/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.738 total time=   6.0s\n",
            "[CV 5/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.738 total time=   3.4s\n",
            "[CV 1/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
            "[CV 1/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.981 total time=   3.3s\n",
            "[CV 2/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   5.1s\n",
            "[CV 3/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   3.7s\n",
            "[CV 4/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.738 total time=   3.5s\n",
            "[CV 5/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.738 total time=   4.3s\n",
            "[CV 1/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.981 total time=   3.5s\n",
            "[CV 2/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   3.6s\n",
            "[CV 3/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   4.1s\n",
            "[CV 4/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.728 total time=   3.5s\n",
            "[CV 5/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.718 total time=   3.5s\n",
            "[CV 1/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.971 total time=   4.0s\n",
            "[CV 2/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   3.5s\n",
            "[CV 3/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.718 total time=   3.6s\n",
            "[CV 5/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.709 total time=   3.7s\n",
            "[CV 1/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.962 total time=   3.3s\n",
            "[CV 2/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   4.1s\n",
            "[CV 3/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   6.6s\n",
            "[CV 4/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.728 total time=   6.3s\n",
            "[CV 5/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.767 total time=   3.9s\n",
            "[CV 1/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=1.000 total time=   3.8s\n",
            "[CV 2/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.750 total time=   4.1s\n",
            "[CV 3/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.524 total time=   3.7s\n",
            "[CV 4/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.718 total time=   3.6s\n",
            "[CV 5/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.738 total time=   6.0s\n",
            "[CV 1/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.990 total time=   3.5s\n",
            "[CV 2/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.524 total time=   4.0s\n",
            "[CV 4/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.728 total time=   3.4s\n",
            "[CV 5/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.777 total time=   3.4s\n",
            "[CV 1/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=1.000 total time=   6.0s\n",
            "[CV 2/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.641 total time=   7.0s\n",
            "[CV 4/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.738 total time=   3.9s\n",
            "[CV 5/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.748 total time=   6.2s\n",
            "[CV 1/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=1.000 total time=   3.8s\n",
            "[CV 2/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.750 total time=   6.1s\n",
            "[CV 3/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.524 total time=   4.0s\n",
            "[CV 4/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.738 total time=   6.1s\n",
            "[CV 5/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.718 total time=   6.0s\n",
            "[CV 1/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.971 total time=   3.5s\n",
            "[CV 2/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.524 total time=   6.0s\n",
            "[CV 4/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.757 total time=   3.5s\n",
            "[CV 5/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.718 total time=   6.0s\n",
            "[CV 1/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.990 total time=   3.3s\n",
            "[CV 2/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.760 total time=   3.4s\n",
            "[CV 3/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.524 total time=   6.0s\n",
            "[CV 4/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.728 total time=   4.7s\n",
            "[CV 5/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.748 total time=   6.3s\n",
            "[CV 1/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.962 total time=   6.2s\n",
            "[CV 2/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.750 total time=   4.2s\n",
            "[CV 3/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.524 total time=   3.6s\n",
            "[CV 4/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.738 total time=   3.5s\n",
            "[CV 5/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.718 total time=   3.9s\n",
            "[CV 1/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.971 total time=   6.2s\n",
            "[CV 2/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.750 total time=   3.5s\n",
            "[CV 3/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.631 total time=   6.0s\n",
            "[CV 4/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.728 total time=   3.4s\n",
            "[CV 5/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.728 total time=   3.5s\n",
            "[CV 1/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.971 total time=   6.2s\n",
            "[CV 2/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.750 total time=   3.3s\n",
            "[CV 3/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.641 total time=   6.0s\n",
            "[CV 4/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.748 total time=   3.4s\n",
            "[CV 5/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.709 total time=   4.7s\n",
            "[CV 1/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=1.000 total time=   6.5s\n",
            "[CV 2/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.750 total time=   3.5s\n",
            "[CV 3/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.524 total time=   4.3s\n",
            "[CV 4/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.709 total time=   3.6s\n",
            "[CV 5/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.738 total time=   6.1s\n",
            "[CV 1/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=1.000 total time=   6.3s\n",
            "[CV 2/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.750 total time=   6.1s\n",
            "[CV 3/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.524 total time=   6.0s\n",
            "[CV 4/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.748 total time=   3.5s\n",
            "[CV 5/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.786 total time=   6.2s\n",
            "[CV 1/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.990 total time=   3.4s\n",
            "[CV 2/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.750 total time=   6.0s\n",
            "[CV 3/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.728 total time=   3.4s\n",
            "[CV 5/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.757 total time=   3.9s\n",
            "[CV 1/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.971 total time=   7.2s\n",
            "[CV 2/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.750 total time=   4.4s\n",
            "[CV 3/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.641 total time=   3.7s\n",
            "[CV 4/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.738 total time=   3.6s\n",
            "[CV 5/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.757 total time=   6.1s\n",
            "[CV 1/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.981 total time=   6.1s\n",
            "[CV 2/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.750 total time=   6.1s\n",
            "[CV 3/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.524 total time=   6.1s\n",
            "[CV 4/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.728 total time=   4.1s\n",
            "[CV 5/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.728 total time=   3.5s\n",
            "[CV 1/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.981 total time=   6.1s\n",
            "[CV 2/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.750 total time=   6.2s\n",
            "[CV 3/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.573 total time=   3.4s\n",
            "[CV 4/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.738 total time=   6.0s\n",
            "[CV 5/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.757 total time=   3.2s\n",
            "[CV 1/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.981 total time=   3.4s\n",
            "[CV 2/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.750 total time=   7.3s\n",
            "[CV 3/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.524 total time=   6.3s\n",
            "[CV 4/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.718 total time=   4.0s\n",
            "[CV 5/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.709 total time=   3.8s\n",
            "[CV 1/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.971 total time=   4.1s\n",
            "[CV 2/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.760 total time=   3.9s\n",
            "[CV 3/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.524 total time=   6.1s\n",
            "[CV 4/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.718 total time=   4.1s\n",
            "[CV 5/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.757 total time=   3.5s\n",
            "[CV 1/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.971 total time=   6.1s\n",
            "[CV 2/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.750 total time=   3.5s\n",
            "[CV 3/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.621 total time=   3.3s\n",
            "[CV 4/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.728 total time=   3.5s\n",
            "[CV 5/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.738 total time=   3.8s\n",
            "[CV 1/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   3.4s\n",
            "[CV 2/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   3.3s\n",
            "[CV 3/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   4.8s\n",
            "[CV 4/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.718 total time=   4.0s\n",
            "[CV 5/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.748 total time=   3.8s\n",
            "[CV 1/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.990 total time=   4.2s\n",
            "[CV 2/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   3.7s\n",
            "[CV 3/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   6.1s\n",
            "[CV 4/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.738 total time=   6.1s\n",
            "[CV 5/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.767 total time=   6.1s\n",
            "[CV 1/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   6.0s\n",
            "[CV 2/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   6.0s\n",
            "[CV 3/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.738 total time=   6.1s\n",
            "[CV 5/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.757 total time=   3.5s\n",
            "[CV 1/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.990 total time=   3.4s\n",
            "[CV 2/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   3.4s\n",
            "[CV 3/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   6.2s\n",
            "[CV 4/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.689 total time=   3.5s\n",
            "[CV 5/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.748 total time=   7.1s\n",
            "[CV 1/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.971 total time=   6.3s\n",
            "[CV 2/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   6.4s\n",
            "[CV 3/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.641 total time=   3.6s\n",
            "[CV 4/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.738 total time=   6.1s\n",
            "[CV 5/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.709 total time=   3.6s\n",
            "[CV 1/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.990 total time=   6.1s\n",
            "[CV 2/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   3.6s\n",
            "[CV 3/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   3.6s\n",
            "[CV 4/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.738 total time=   6.0s\n",
            "[CV 5/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.718 total time=   3.5s\n",
            "[CV 1/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.981 total time=   3.4s\n",
            "[CV 2/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   4.0s\n",
            "[CV 3/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   3.4s\n",
            "[CV 4/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   3.3s\n",
            "[CV 5/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.757 total time=   6.0s\n",
            "[CV 1/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.952 total time=   7.1s\n",
            "[CV 2/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   4.4s\n",
            "[CV 3/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
          ]
        }
      ]
    }
  ]
}